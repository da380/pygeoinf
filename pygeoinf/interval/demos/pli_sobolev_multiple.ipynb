{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48255d4b",
   "metadata": {},
   "source": [
    "# Probabilistic Linear Inference on Direct Sum of Sobolev Spaces\n",
    "\n",
    "## The Continuous Problem\n",
    "\n",
    "We consider the problem of inferring an unknown model parameter $m \\in \\mathcal{M} = H^s([0,1]) \\oplus H^s([0,1])$ from noisy observations $\\mathbf{\\tilde{d}} \\in \\mathcal{D} \\cong \\mathbb{R}^{N_d}$.\n",
    "\n",
    "The model space is a **direct sum of two Sobolev spaces**, representing two different physical parameters (e.g., $v_p$ and $v_s$ seismic velocities). This structure allows us to:\n",
    "- Infer multiple model components simultaneously\n",
    "- Respect different smoothness constraints for each component\n",
    "- Target properties from specific components or combinations thereof\n",
    "\n",
    "The relationship between the model and the data is described by a forward operator:\n",
    "$$\n",
    "G: \\mathcal{M} \\to \\mathcal{D}, \\quad [G(m)]_i = \\langle K_i, m \\rangle_{\\mathcal{M}}\n",
    "$$\n",
    "assumed to be linear and bounded. The observed data is modeled as:\n",
    "$$\n",
    "\\mathbf{\\tilde{d}} = G(m) + \\bm{\\eta},\n",
    "$$\n",
    "where $\\bm{\\eta} \\sim \\mathcal{N}(0, \\mathbf{C}_{\\mathcal{D}})$ represents additive Gaussian noise.\n",
    "\n",
    "To regularize this ill‚Äëposed inverse problem, we adopt a Bayesian framework with a Gaussian prior:\n",
    "$$\n",
    "m \\sim \\mu_{\\mathcal{M}}^0 := \\mathcal{N}(m_0, C_0),\n",
    "$$\n",
    "where $m_0 = (m_{0,vp}, m_{0,vs}) \\in \\mathcal{M}$ is the prior mean, and $C_0$ is a block-diagonal covariance operator respecting the direct sum structure.\n",
    "\n",
    "**Sobolev Space Direct Sum**  \n",
    "The direct sum structure means:\n",
    "- Each component lives in its own Sobolev space with boundary conditions\n",
    "- The inner product is the sum of component inner products\n",
    "- Covariance operators can be block-diagonal or coupled\n",
    "\n",
    "Bayesian inference asks for a specific property of the true model. Let \n",
    "$$\n",
    "\\mathcal{T} \\colon \\mathcal{M} \\to \\mathcal{P}, \\quad [\\mathcal{T}(m)]_i = \\langle T_i, m \\rangle_{\\mathcal{M}}\n",
    "$$\n",
    "be a linear and bounded mapping that extracts properties. By pushing the posterior measure through $\\mathcal{T}$ we obtain the property measure:\n",
    "$$\n",
    "\\mu_{\\mathcal{P}} = \\mathcal{T}_*\\mu_{\\mathcal{M}}^{\\mathbf{\\tilde{d}}}\n",
    "$$\n",
    "\n",
    "## Continuous Solution\n",
    "\n",
    "Using a Bayesian update we obtain the posterior measure on the model space $\\mu_{\\mathcal{M}}^{\\mathbf{\\tilde{d}}} = \\mathcal{N}(\\tilde{m}, C_{\\mathcal{M}})$ where:\n",
    "$$\n",
    "\\boxed{\n",
    "C_{\\mathcal{M}} = \\left( G^* \\mathbf{C}_{\\mathcal{D}}^{-1} G + C_0^{-1} \\right)^{-1}, \\quad\n",
    "\\tilde{m} = C_{\\mathcal{M}} \\left( G^* \\mathbf{C}_{\\mathcal{D}}^{-1} \\mathbf{\\tilde{d}} + C_0^{-1}m_0 \\right).\n",
    "}\n",
    "$$\n",
    "\n",
    "Then we can obtain the property measure from the model posterior $\\mu_{\\mathcal{P}} = \\mathcal{N}(\\mathbf{\\tilde{p}}, \\mathbf{C}_{\\mathcal{P}})$:\n",
    "$$\n",
    "\\boxed{\n",
    "\\mathbf{\\tilde{p}} = \\mathcal{T}(\\tilde{m}), \\quad \\mathbf{C}_{\\mathcal{P}} = \\mathcal{T} C_{\\mathcal{M}} \\mathcal{T}^*.\n",
    "}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f05db0b",
   "metadata": {},
   "source": [
    "### Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dec6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pygeoinf.interval.function_providers import NormalModesProvider, BumpFunctionProvider, NullFunctionProvider\n",
    "from pygeoinf.interval.interval_domain import IntervalDomain\n",
    "from pygeoinf.interval import Sobolev, Laplacian, Lebesgue, BoundaryConditions, SobolevSpaceDirectSum\n",
    "from pygeoinf.hilbert_space import EuclideanSpace\n",
    "from pygeoinf.interval.operators import SOLAOperator\n",
    "from pygeoinf.interval.functions import Function\n",
    "from pygeoinf.gaussian_measure import GaussianMeasure\n",
    "import matplotlib.pyplot as plt\n",
    "from pygeoinf import RowLinearOperator\n",
    "import numpy as np\n",
    "from pygeoinf.interval.operators import InverseLaplacian\n",
    "from pygeoinf.forward_problem import LinearForwardProblem\n",
    "from pygeoinf.linear_bayesian import LinearBayesianInference\n",
    "from pygeoinf.linear_solvers import CholeskySolver\n",
    "import seaborn as sns\n",
    "\n",
    "# Set-up folder for saving figures\n",
    "import os\n",
    "figures_folder = 'pli_sobolev_figures'\n",
    "if not os.path.exists(figures_folder):\n",
    "    os.makedirs(figures_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a29b2c",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "Control the inference workflow with the flag below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6bb3d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CONFIGURATION FLAGS\n",
    "# =============================================================================\n",
    "\n",
    "# Set to True to compute and visualize the model posterior\n",
    "# Set to False to skip model posterior and go directly to property posterior\n",
    "COMPUTE_MODEL_POSTERIOR = False\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  COMPUTE_MODEL_POSTERIOR = {COMPUTE_MODEL_POSTERIOR}\")\n",
    "if COMPUTE_MODEL_POSTERIOR:\n",
    "    print(\"  ‚Üí Will compute model posterior, then push to property space\")\n",
    "else:\n",
    "    print(\"  ‚Üí Will skip model posterior and compute property posterior directly\")\n",
    "    print(\"  ‚Üí Expected speedup: ~4-5x faster\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ef3b06",
   "metadata": {},
   "source": [
    "## Creating the spaces\n",
    "\n",
    "We first create the function domain as an interval. The model space will be a **direct sum of two Sobolev spaces** $\\mathcal{M} = H^s([0,1]) \\oplus H^s([0,1])$, representing two different physical parameters (e.g., $v_p$ and $v_s$ seismic velocities)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b53fb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function domain and spaces\n",
    "function_domain = IntervalDomain(0, 1)\n",
    "\n",
    "# Create Laplacian operator for Sobolev space definition\n",
    "M_laplacian = Lebesgue(0, function_domain, basis=None)\n",
    "bcs = BoundaryConditions(bc_type='mixed_neumann_dirichlet')\n",
    "laplacian = Laplacian(M_laplacian, bcs, 1.0, method='spectral', dofs=100)\n",
    "\n",
    "# Create two Sobolev spaces\n",
    "s, k, L = 1, 10, laplacian\n",
    "M_vp = Sobolev(0, function_domain, s, k, L, basis=None)\n",
    "M_vs = Sobolev(0, function_domain, s, k, L, basis=None)\n",
    "\n",
    "# Take their direct sum\n",
    "M = SobolevSpaceDirectSum([M_vp, M_vs])\n",
    "\n",
    "# Data and property spaces\n",
    "N_d = 50  # number of data points\n",
    "D = EuclideanSpace(N_d)  # data space\n",
    "N_p = 20  # number of property points\n",
    "P = EuclideanSpace(N_p)  # property space\n",
    "\n",
    "print(f\"Model space: Direct sum of two Sobolev H^{s} spaces\")\n",
    "print(f\"Data space dimension: {N_d}\")\n",
    "print(f\"Property space dimension: {N_p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2198aae9",
   "metadata": {},
   "source": [
    "## Create mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1f0486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create forward and property mappings\n",
    "width = 0.2  # width of the bump target functions\n",
    "centers = np.linspace(function_domain.a + width / 2, function_domain.b - width / 2, N_p)\n",
    "\n",
    "# Create a normal modes provider for the forward operator\n",
    "# Note: The random_state is set to ensure reproducibility of results\n",
    "normal_modes_provider_vp = NormalModesProvider(\n",
    "    M_vp,\n",
    "    n_modes_range=(1, 50),\n",
    "    coeff_range=(-5, 5),\n",
    "    gaussian_width_percent_range=(1, 5),\n",
    "    freq_range=(0.1, 20),\n",
    "    random_state=2,\n",
    ")\n",
    "normal_modes_provider_vs = NormalModesProvider(\n",
    "    M_vs,\n",
    "    n_modes_range=(1, 50),\n",
    "    coeff_range=(-5, 5),\n",
    "    gaussian_width_percent_range=(1, 5),\n",
    "    freq_range=(0.1, 20),\n",
    "    random_state=3,\n",
    ")\n",
    "G_vp = SOLAOperator(M_vp, D, normal_modes_provider_vp)\n",
    "G_vs = SOLAOperator(M_vs, D, normal_modes_provider_vs)\n",
    "\n",
    "# Combine the two forward operators into one mapping from the direct sum space\n",
    "G = RowLinearOperator([G_vp, G_vs])\n",
    "\n",
    "# Now we create targets for each property\n",
    "target_provider_vp = BumpFunctionProvider(M_vp, centers=centers, default_width=width)\n",
    "target_provider_vs = NullFunctionProvider(M_vs)  # No target for vs\n",
    "\n",
    "# Create SOLA property operators\n",
    "T_vp = SOLAOperator(M_vp, P, target_provider_vp)\n",
    "T_vs = SOLAOperator(M_vs, P, target_provider_vs)\n",
    "\n",
    "# Combine them into one property operator from the direct sum space\n",
    "T = RowLinearOperator([T_vp, T_vs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717fdcc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensitivity Kernels figure\n",
    "fig_title = \"Sensitivity Kernels (vp & vs - Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "x = np.linspace(function_domain.a, function_domain.b, 1000)\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "\n",
    "# Create two vertically stacked subplots: top for G_vp, bottom for G_vs\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8), dpi=200, sharex=True)\n",
    "\n",
    "# Top subplot: G_vp kernels\n",
    "axs[0].set_title(r\"Sensitivity Kernels: $G_{vp}$ (Sobolev)\", fontsize=16)\n",
    "for i in range(N_d):\n",
    "    axs[0].plot(\n",
    "        x,\n",
    "        G_vp.get_kernel(i).evaluate(x),\n",
    "        color='tab:blue',\n",
    "        alpha=0.35,\n",
    "        linewidth=1.2,\n",
    "    )\n",
    "axs[0].set_ylabel(\"Kernel Value\", fontsize=12)\n",
    "axs[0].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "# Bottom subplot: G_vs kernels\n",
    "axs[1].set_title(r\"Sensitivity Kernels: $G_{vs}$ (Sobolev)\", fontsize=16)\n",
    "for i in range(N_d):\n",
    "    axs[1].plot(\n",
    "        x,\n",
    "        G_vs.get_kernel(i).evaluate(x),\n",
    "        color='tab:orange',\n",
    "        alpha=0.35,\n",
    "        linewidth=1.2,\n",
    "    )\n",
    "axs[1].set_xlabel(r\"$x$\", fontsize=12)\n",
    "axs[1].set_ylabel(\"Kernel Value\", fontsize=12)\n",
    "axs[1].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2b11b3",
   "metadata": {},
   "source": [
    "### Sensitivity Kernels Visualization\n",
    "\n",
    "The sensitivity kernels $K_i(x)$ define how sensitive each data point $d_i$ is to the model $m(x)$ at different spatial locations. These kernels are crucial for understanding:\n",
    "\n",
    "- **Coverage**: Where in the domain we have observational sensitivity\n",
    "- **Resolution**: How localized our measurements are\n",
    "- **Redundancy**: Overlapping kernels indicate multiple measurements of similar information\n",
    "\n",
    "Regions where many kernels overlap (appearing darker) represent areas of high observational density, while sparse regions indicate limited sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5c4b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Kernels figure (vp & vs)\n",
    "fig_title = \"Target Kernels (vp & vs - Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "x = np.linspace(function_domain.a, function_domain.b, 1000)\n",
    "\n",
    "# Two vertically stacked subplots: top for T_vp, bottom for T_vs\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8), dpi=200, sharex=True)\n",
    "\n",
    "# Top: T_vp\n",
    "axs[0].set_title(r\"Target Kernels: $T_{vp}$ (Sobolev)\", fontsize=16)\n",
    "for i in range(N_p):\n",
    "    axs[0].plot(\n",
    "        x,\n",
    "        T_vp.get_kernel(i).evaluate(x),\n",
    "        color='tab:blue',\n",
    "        alpha=0.6,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "axs[0].set_ylabel(\"Kernel Value\", fontsize=12)\n",
    "axs[0].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "# Bottom: T_vs\n",
    "axs[1].set_title(r\"Target Kernels: $T_{vs}$ (Sobolev)\", fontsize=16)\n",
    "for i in range(N_p):\n",
    "    axs[1].plot(\n",
    "        x,\n",
    "        T_vs.get_kernel(i).evaluate(x),\n",
    "        color='tab:orange',\n",
    "        alpha=0.6,\n",
    "        linewidth=1.5,\n",
    "    )\n",
    "axs[1].set_xlabel(r\"$x$\", fontsize=12)\n",
    "axs[1].set_ylabel(\"Kernel Value\", fontsize=12)\n",
    "axs[1].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7140705c",
   "metadata": {},
   "source": [
    "### Target Kernels for Property Extraction\n",
    "\n",
    "The target kernels $T_i(x)$ define the properties we want to infer about the model. Each kernel extracts a local average of the model around its center location. These bump functions allow us to:\n",
    "\n",
    "- **Localize**: Extract spatially localized properties\n",
    "- **Regularize**: Smooth local estimates through spatial averaging  \n",
    "- **Focus**: Target specific regions of interest in the domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3eaf39",
   "metadata": {},
   "source": [
    "## Synthetic Data Generation\n",
    "\n",
    "### True Model Construction\n",
    "\n",
    "We create synthetic \"true\" models for both components:\n",
    "- $\\bar{m}_{vp}(x) = \\exp(-((x - 0.5)/0.5)^2) \\sin(5\\pi x) + x$\n",
    "- $\\bar{m}_{vs}(x) = \\exp(-((x - 0.5)/0.5)^2) \\sin(10\\pi x) + x$\n",
    "\n",
    "These models include:\n",
    "- **Global trend**: Linear component $x$ \n",
    "- **Localized oscillation**: Gaussian-windowed sine waves with different frequencies\n",
    "- **Smooth envelope**: Exponential decay for spatial localization\n",
    "\n",
    "From these true models, we generate synthetic observations $\\mathbf{\\bar{d}} = G(\\bar{m})$ and add noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d08f5b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the synthetic true models for vp and vs\n",
    "m_bar_vp = Function(M_vp, evaluate_callable=lambda x: np.exp(-((x - function_domain.center)/0.5)**2) * np.sin(5 * np.pi * x) + x)\n",
    "m_bar_vs = Function(M_vs, evaluate_callable=lambda x: np.exp(-((x - function_domain.center)/0.5)**2) * np.sin(10 * np.pi * x) + x)\n",
    "m_bar = [m_bar_vp, m_bar_vs]\n",
    "\n",
    "x = np.linspace(function_domain.a, function_domain.b, 1000)\n",
    "\n",
    "# True model plots: two stacked subplots (vp on top, vs on bottom)\n",
    "fig_title = \"True Models (vp & vs - Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"muted\", color_codes=True)\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 8), dpi=200, sharex=True)\n",
    "\n",
    "# Top: vp\n",
    "axs[0].plot(x, m_bar_vp.evaluate(x), color='tab:red', linewidth=2.5, label=r'$\\bar{m}_{vp}(x)$')\n",
    "axs[0].set_title(r\"True Model: $\\bar{m}_{vp}(x)$ (Sobolev)\", fontsize=16)\n",
    "axs[0].set_ylabel('Model Value', fontsize=12)\n",
    "axs[0].legend(fontsize=12)\n",
    "axs[0].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "# Bottom: vs\n",
    "axs[1].plot(x, m_bar_vs.evaluate(x), color='tab:blue', linewidth=2.5, label=r'$\\bar{m}_{vs}(x)$')\n",
    "axs[1].set_title(r\"True Model: $\\bar{m}_{vs}(x)$ (Sobolev)\", fontsize=16)\n",
    "axs[1].set_xlabel(r\"$x$\", fontsize=12)\n",
    "axs[1].set_ylabel('Model Value', fontsize=12)\n",
    "axs[1].legend(fontsize=12)\n",
    "axs[1].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Generate synthetic observations\n",
    "print(\"Generating synthetic data...\")\n",
    "d_bar = G(m_bar)\n",
    "\n",
    "# Compute noise based on the numeric array\n",
    "noise_level = 0.1 * np.max(d_bar)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "d_tilde = d_bar + np.random.normal(0, noise_level, d_bar.shape)\n",
    "\n",
    "print(f\"Signal-to-noise ratio: {np.max(d_bar) / noise_level:.1f}\")\n",
    "print(f\"Number of observations: {len(d_tilde)}\")\n",
    "\n",
    "# Data comparison plot\n",
    "fig_title = \"Synthetic Observations (Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "plt.figure(figsize=(12, 4), dpi=200)\n",
    "data_indices = np.arange(len(d_bar))\n",
    "\n",
    "# Plot connection lines between true and noisy data\n",
    "for i in range(len(d_bar)):\n",
    "    plt.plot([i, i], [d_bar[i], d_tilde[i]], color='gray', alpha=0.3, linewidth=0.8)\n",
    "\n",
    "# Plot the data points\n",
    "plt.scatter(data_indices, d_tilde, label='Noisy Observations',\n",
    "           color='tab:blue', alpha=0.7, marker='o', s=25, edgecolors='white', linewidths=0.5)\n",
    "plt.scatter(data_indices, d_bar, label='True Data',\n",
    "           color='tab:red', alpha=0.8, marker='x', s=30, linewidths=1.5)\n",
    "\n",
    "plt.xlabel('Observation Index', fontsize=16)\n",
    "plt.ylabel('Data Value', fontsize=16)\n",
    "plt.title('Synthetic Observations: Truth vs. Noisy Measurements', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True, linestyle=':', alpha=0.4)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5d826d",
   "metadata": {},
   "source": [
    "## Bayesian Setup: Data and Prior Measures\n",
    "\n",
    "### Data Noise Model\n",
    "\n",
    "We model the measurement noise as independent Gaussian with known variance:\n",
    "$$\\mathbf{\\eta} \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{C}_{\\mathcal{D}})$$\n",
    "\n",
    "where $\\mathbf{C}_{\\mathcal{D}} = \\sigma^2 \\mathbf{I}$ represents uncorrelated measurement errors.\n",
    "\n",
    "**Important distinction**: \n",
    "- **Data error measure**: $\\mathcal{N}(\\mathbf{0}, \\mathbf{C}_{\\mathcal{D}})$ - represents the noise model for `LinearForwardProblem`\n",
    "- **Data measure**: $\\mathcal{N}(\\mathbf{\\tilde{d}}, \\mathbf{C}_{\\mathcal{D}})$ - represents the observed data for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e618eb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data noise covariance\n",
    "noise_variance = (0.1 * np.max(d_tilde))**2  # 10% of peak signal\n",
    "C_D_matrix = noise_variance * np.eye(N_d)\n",
    "\n",
    "print(f\"Data noise standard deviation: {np.sqrt(noise_variance):.4f}\")\n",
    "print(f\"Relative noise level: {100 * np.sqrt(noise_variance) / np.max(d_tilde):.1f}%\")\n",
    "\n",
    "# Create data NOISE measure (zero mean) for LinearForwardProblem\n",
    "gaussian_D_noise = GaussianMeasure.from_covariance_matrix(D, C_D_matrix, expectation=np.zeros(N_d))\n",
    "\n",
    "# Create data measure (with observed data mean) for visualization\n",
    "gaussian_D = GaussianMeasure.from_covariance_matrix(D, C_D_matrix, expectation=d_tilde)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e130d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the data measure\n",
    "mean_values = gaussian_D.expectation\n",
    "std_values = np.sqrt(noise_variance)\n",
    "\n",
    "fig_title = \"Data Likelihood Distribution (Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "plt.figure(figsize=(12, 4), dpi=200)\n",
    "data_indices = np.arange(len(mean_values))\n",
    "\n",
    "plt.scatter(data_indices, mean_values, label='Observed Data', color='tab:blue', alpha=0.8, s=30)\n",
    "plt.errorbar(data_indices, mean_values, yerr=std_values, fmt='none',\n",
    "            color='tab:blue', alpha=0.5, capsize=2, capthick=1,\n",
    "            label='¬±1œÉ Uncertainty')\n",
    "\n",
    "plt.title(\"Data Likelihood: Observations with Uncertainty\", fontsize=18)\n",
    "plt.xlabel('Observation Index', fontsize=16)\n",
    "plt.ylabel('Data Value', fontsize=16)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True, linestyle=':', alpha=0.4)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1b8594",
   "metadata": {},
   "source": [
    "### Prior Model for Direct Sum of Sobolev Spaces\n",
    "\n",
    "We assign a Gaussian prior measure to regularize the inverse problem:\n",
    "$$m \\sim \\mu_{\\mathcal{M}}^0 = \\mathcal{N}(m_0, C_0)$$\n",
    "\n",
    "**Prior Mean**: $m_{0,vp}(x) = x$ and $m_{0,vs}(x) = \\sin(x)$\n",
    "\n",
    "**Prior Covariance**: We use inverse Laplacian operators for each component:\n",
    "$$C_{0,vp} = (-\\alpha_{vp} \\Delta)^{-1}, \\quad C_{0,vs} = (-\\alpha_{vs} \\Delta)^{-1}$$\n",
    "\n",
    "where $\\alpha > 0$ controls the correlation length scale. The full covariance is block-diagonal respecting the direct sum structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a13cc69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prior measure parameters\n",
    "alpha_vp = 0.1  # Amplifier -> the bigger -> the smoother\n",
    "alpha_vs = 0.05\n",
    "C_0_vp = InverseLaplacian(M_vp, bcs, alpha_vp, method='fem', dofs=100)\n",
    "C_0_vs = InverseLaplacian(M_vs, bcs, alpha_vs, method='fem', dofs=100)\n",
    "\n",
    "# Prior means\n",
    "m_0_vp = Function(M_vp, evaluate_callable=lambda x: x)\n",
    "m_0_vs = Function(M_vs, evaluate_callable=lambda x: np.sin(x))\n",
    "\n",
    "print(f\"Prior covariance vp: (-{alpha_vp}Œî)‚Åª¬π with {bcs.type} boundary conditions\")\n",
    "print(f\"Prior covariance vs: (-{alpha_vs}Œî)‚Åª¬π with {bcs.type} boundary conditions\")\n",
    "\n",
    "# Set up Karhunen-Lo√®ve expansion for efficient sampling\n",
    "K = 100  # Number of KL terms\n",
    "print(f\"Using {K} Karhunen-Lo√®ve terms for prior sampling\")\n",
    "\n",
    "# Create Gaussian measures on each component space\n",
    "M_prior_vp = GaussianMeasure.from_spectral(C_0_vp, expectation=m_0_vp, n_modes=K)\n",
    "M_prior_vs = GaussianMeasure.from_spectral(C_0_vs, expectation=m_0_vs, n_modes=K)\n",
    "\n",
    "# Combine into a joint prior measure\n",
    "M_prior = GaussianMeasure.from_direct_sum([M_prior_vp, M_prior_vs])\n",
    "\n",
    "# Display eigenvalue decay\n",
    "eigenvals_vp = C_0_vp.get_eigenvalues(range(50))\n",
    "eigenvals_vs = C_0_vs.get_eigenvalues(range(50))\n",
    "print(f\"Eigenvalue range (vp): [{eigenvals_vp[-1]:.2e}, {eigenvals_vp[0]:.2e}]\")\n",
    "print(f\"Spectral decay rate (vp): {eigenvals_vp[0]/eigenvals_vp[-1]:.1e}\")\n",
    "print(f\"Eigenvalue range (vs): [{eigenvals_vs[-1]:.2e}, {eigenvals_vs[0]:.2e}]\")\n",
    "print(f\"Spectral decay rate (vs): {eigenvals_vs[0]/eigenvals_vs[-1]:.1e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2348e74e",
   "metadata": {},
   "source": [
    "### Prior Measure Visualization\n",
    "\n",
    "The Gaussian prior encodes our beliefs about the model before seeing data. We visualize this for both components:\n",
    "- **Samples**: Random realizations from the prior\n",
    "- **Mean**: Expected model structure  \n",
    "- **Uncertainty**: Point-wise standard deviation showing prior variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b3571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the prior measure (two subplots for vp and vs)\n",
    "fig_title = \"Prior Measure on Model Space (vp & vs - Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "# Parameters for visualization\n",
    "num_samples = 15\n",
    "sample_color_vp = 'tab:blue'\n",
    "sample_color_vs = 'tab:orange'\n",
    "mean_color = 'tab:green'\n",
    "std_color_vp = 'tab:blue'\n",
    "std_color_vs = 'tab:orange'\n",
    "\n",
    "# Create two vertically stacked subplots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(12, 10), dpi=200, sharex=True)\n",
    "\n",
    "# Plot prior samples for vp and vs\n",
    "print(\"Drawing samples from prior measure...\")\n",
    "for i in range(num_samples):\n",
    "    sample = M_prior.sample()\n",
    "    sample_vp, sample_vs = sample\n",
    "\n",
    "    # Top subplot: vp\n",
    "    axs[0].plot(x, sample_vp.evaluate(x), color=sample_color_vp, alpha=0.25, linewidth=1,\n",
    "                label='Prior Samples' if i == 0 else \"\")\n",
    "\n",
    "    # Bottom subplot: vs\n",
    "    axs[1].plot(x, sample_vs.evaluate(x), color=sample_color_vs, alpha=0.25, linewidth=1,\n",
    "                label='Prior Samples' if i == 0 else \"\")\n",
    "\n",
    "# Compute and plot uncertainty bounds for vp\n",
    "print(\"Computing prior uncertainty (vp)...\")\n",
    "std_vp = M_vp.zero\n",
    "for i, eigenvalue in enumerate(C_0_vp.get_eigenvalues(range(K))):\n",
    "    eigenfunction = C_0_vp.get_eigenfunction(i)\n",
    "    std_vp += eigenvalue * eigenfunction * eigenfunction\n",
    "std_values_vp = np.sqrt(std_vp.evaluate(x))\n",
    "\n",
    "# Compute and plot uncertainty bounds for vs\n",
    "print(\"Computing prior uncertainty (vs)...\")\n",
    "std_vs = M_vs.zero\n",
    "for i, eigenvalue in enumerate(C_0_vs.get_eigenvalues(range(K))):\n",
    "    eigenfunction = C_0_vs.get_eigenfunction(i)\n",
    "    std_vs += eigenvalue * eigenfunction * eigenfunction\n",
    "std_values_vs = np.sqrt(std_vs.evaluate(x))\n",
    "\n",
    "# Plot prior mean and uncertainty for vp (top subplot)\n",
    "mean_vp, mean_vs = M_prior.expectation\n",
    "mean_values_vp = mean_vp.evaluate(x)\n",
    "axs[0].plot(x, mean_values_vp, color=mean_color, linewidth=3, label='Prior Mean', zorder=10)\n",
    "axs[0].fill_between(x, mean_values_vp - 2*std_values_vp, mean_values_vp + 2*std_values_vp,\n",
    "                     color=std_color_vp, alpha=0.15, label='¬±2œÉ Band')\n",
    "axs[0].plot(x, mean_values_vp + 2*std_values_vp, color=std_color_vp, linestyle='--',\n",
    "            alpha=0.7, linewidth=1.5, label='¬±2œÉ Boundaries')\n",
    "axs[0].plot(x, mean_values_vp - 2*std_values_vp, color=std_color_vp, linestyle='--',\n",
    "            alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# Plot true vp model\n",
    "axs[0].plot(x, m_bar_vp.evaluate(x), color='tab:red', linewidth=2.5,\n",
    "            label='True Model', linestyle='--', zorder=9)\n",
    "\n",
    "axs[0].set_title(r\"Prior Measure: $v_p$ component (Sobolev)\", fontsize=16)\n",
    "axs[0].set_ylabel(\"Model Value\", fontsize=14)\n",
    "axs[0].legend(fontsize=11, loc='upper right')\n",
    "axs[0].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "# Plot prior mean and uncertainty for vs (bottom subplot)\n",
    "mean_values_vs = mean_vs.evaluate(x)\n",
    "axs[1].plot(x, mean_values_vs, color=mean_color, linewidth=3, label='Prior Mean', zorder=10)\n",
    "axs[1].fill_between(x, mean_values_vs - 2*std_values_vs, mean_values_vs + 2*std_values_vs,\n",
    "                     color=std_color_vs, alpha=0.15, label='¬±2œÉ Band')\n",
    "axs[1].plot(x, mean_values_vs + 2*std_values_vs, color=std_color_vs, linestyle='--',\n",
    "            alpha=0.7, linewidth=1.5, label='¬±2œÉ Boundaries')\n",
    "axs[1].plot(x, mean_values_vs - 2*std_values_vs, color=std_color_vs, linestyle='--',\n",
    "            alpha=0.7, linewidth=1.5)\n",
    "\n",
    "# Plot true vs model\n",
    "axs[1].plot(x, m_bar_vs.evaluate(x), color='tab:red', linewidth=2.5,\n",
    "            label='True Model', linestyle='--', zorder=9)\n",
    "\n",
    "axs[1].set_title(r\"Prior Measure: $v_s$ component (Sobolev)\", fontsize=16)\n",
    "axs[1].set_xlabel(r\"$x$\", fontsize=14)\n",
    "axs[1].set_ylabel(\"Model Value\", fontsize=14)\n",
    "axs[1].legend(fontsize=11, loc='upper right')\n",
    "axs[1].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404d9519",
   "metadata": {},
   "source": [
    "## Property Prior Analysis\n",
    "\n",
    "### Push-forward of Model Prior\n",
    "\n",
    "The property prior is obtained by pushing the model prior through the target operator:\n",
    "$$\\mu_{\\mathcal{P}}^0 = \\mathcal{T}_* \\mu_{\\mathcal{M}}^0 = \\mathcal{N}(\\mathcal{T}(m_0), \\mathcal{T} C_0 \\mathcal{T}^*)$$\n",
    "\n",
    "This gives us prior beliefs about the local properties before incorporating data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d111ea17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute property prior by pushing model prior through target operator\n",
    "print(\"Computing property prior measure...\")\n",
    "prior_P = M_prior.affine_mapping(operator=T)\n",
    "std_P = np.sqrt(np.diag(prior_P.covariance.matrix(dense=True, parallel=True, n_jobs=8)))\n",
    "\n",
    "print(f\"Property prior computed for {N_p} target locations\")\n",
    "print(f\"Property uncertainty range: [{std_P.min():.3f}, {std_P.max():.3f}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30471c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize property prior\n",
    "fig_title = \"Property Prior Distribution (Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "plt.figure(figsize=(12, 5), dpi=200)\n",
    "\n",
    "# Property prior mean and uncertainty\n",
    "mean_prop = T(M_prior.expectation)\n",
    "plt.errorbar(centers, mean_prop, yerr=2*std_P, fmt='o', color='tab:blue',\n",
    "            alpha=0.7, capsize=4, capthick=2, markersize=6,\n",
    "            label='Property Prior (mean ¬±2œÉ)')\n",
    "plt.fill_between(centers, mean_prop - 2*std_P, mean_prop + 2*std_P,\n",
    "                color='tab:blue', alpha=0.15)\n",
    "\n",
    "# True property values\n",
    "true_props = T(m_bar)\n",
    "plt.scatter(centers, true_props, label='True Properties',\n",
    "           color='tab:red', marker='x', s=100, alpha=0.9, linewidths=3, zorder=10)\n",
    "\n",
    "# Sample from property prior\n",
    "sampled_props = prior_P.sample()\n",
    "plt.scatter(centers, sampled_props, label='Prior Sample',\n",
    "           color='tab:green', marker='s', s=50, alpha=0.7, zorder=5)\n",
    "\n",
    "plt.xlabel('Target Location', fontsize=16)\n",
    "plt.ylabel('Property Value', fontsize=16)\n",
    "plt.title('Property Prior: Beliefs Before Data', fontsize=18)\n",
    "plt.legend(fontsize=14)\n",
    "plt.grid(True, linestyle=':', alpha=0.4)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"True properties range: [{true_props.min():.3f}, {true_props.max():.3f}]\")\n",
    "print(f\"How many true properties are within prior ¬±2œÉ: {np.sum(np.abs(true_props - mean_prop) <= 2*std_P)}/{len(true_props)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a03d39",
   "metadata": {},
   "source": [
    "## Bayesian Update: Computing the Posterior\n",
    "\n",
    "### Model Posterior\n",
    "\n",
    "The Bayesian update combines prior beliefs with data likelihood to give the posterior:\n",
    "$$\\mu_{\\mathcal{M}}^{\\mathbf{\\tilde{d}}} = \\mathcal{N}(\\tilde{m}, C_{\\mathcal{M}})$$\n",
    "\n",
    "where:\n",
    "- **Posterior covariance**: $C_{\\mathcal{M}} = (G^* \\mathbf{C}_{\\mathcal{D}}^{-1} G + C_0^{-1})^{-1}$\n",
    "- **Posterior mean**: $\\tilde{m} = C_{\\mathcal{M}}(G^* \\mathbf{C}_{\\mathcal{D}}^{-1} \\mathbf{\\tilde{d}} + C_0^{-1} m_0)$\n",
    "\n",
    "This represents our updated beliefs about the model after incorporating the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0407a30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bayesian inference setup and posterior computation\n",
    "import time\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"BAYESIAN INFERENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Common setup\n",
    "t0 = time.time()\n",
    "print(\"\\nSetting up Bayesian inference...\")\n",
    "forward_problem = LinearForwardProblem(G, data_error_measure=gaussian_D_noise)\n",
    "bayesian_inference = LinearBayesianInference(forward_problem, M_prior, T)\n",
    "solver = CholeskySolver(parallel=True, n_jobs=8)\n",
    "t1 = time.time()\n",
    "print(f\"   Setup time: {t1-t0:.3f}s\")\n",
    "\n",
    "if COMPUTE_MODEL_POSTERIOR:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WORKFLOW 1: Model Posterior ‚Üí Property Posterior\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Compute model posterior\n",
    "    print(\"\\n1. Computing model posterior measure...\")\n",
    "    t2 = time.time()\n",
    "    posterior_model = bayesian_inference.model_posterior_measure(d_tilde, solver)\n",
    "    t3 = time.time()\n",
    "    print(f\"   Time: {t3-t2:.3f}s\")\n",
    "\n",
    "    # Extract dense matrix\n",
    "    print(\"\\n2. Extracting dense covariance matrix...\")\n",
    "    t4 = time.time()\n",
    "    C_M_matrix = posterior_model.covariance.matrix(dense=True, parallel=True, n_jobs=8)\n",
    "    t5 = time.time()\n",
    "    print(f\"   Time: {t5-t4:.3f}s\")\n",
    "\n",
    "    # Create sampling-capable measure\n",
    "    print(\"\\n3. Creating sampling-capable measure...\")\n",
    "    t6 = time.time()\n",
    "    mu_M = GaussianMeasure.from_covariance_matrix(M, C_M_matrix, expectation=posterior_model.expectation)\n",
    "    m_tilde = mu_M.expectation\n",
    "    t7 = time.time()\n",
    "    print(f\"   Time: {t7-t6:.3f}s\")\n",
    "\n",
    "    # Compute property posterior by pushing model posterior\n",
    "    print(\"\\n4. Computing property posterior (via model)...\")\n",
    "    t8 = time.time()\n",
    "    property_posterior = mu_M.affine_mapping(operator=T)\n",
    "    p_tilde = property_posterior.expectation\n",
    "    cov_P_matrix = property_posterior.covariance.matrix(dense=True, parallel=True, n_jobs=8)\n",
    "    t9 = time.time()\n",
    "    print(f\"   Time: {t9-t8:.3f}s\")\n",
    "\n",
    "    total_time = t9 - t0\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"WORKFLOW 1 TOTAL TIME: {total_time:.3f}s\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"\\nBreakdown:\")\n",
    "    print(f\"  Setup:             {(t1-t0)/total_time*100:5.1f}%  ({t1-t0:.3f}s)\")\n",
    "    print(f\"  Model posterior:   {(t3-t2)/total_time*100:5.1f}%  ({t3-t2:.3f}s)\")\n",
    "    print(f\"  Dense matrix:      {(t5-t4)/total_time*100:5.1f}%  ({t5-t4:.3f}s)\")\n",
    "    print(f\"  Measure creation:  {(t7-t6)/total_time*100:5.1f}%  ({t7-t6:.3f}s)\")\n",
    "    print(f\"  Property posterior:{(t9-t8)/total_time*100:5.1f}%  ({t9-t8:.3f}s)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"WORKFLOW 2: Property Posterior Directly (Skip Model)\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "    # Compute model posterior mean only (no dense covariance)\n",
    "    print(\"\\n1. Computing model posterior (mean only)...\")\n",
    "    t2 = time.time()\n",
    "    posterior_model = bayesian_inference.model_posterior_measure(d_tilde, solver)\n",
    "    m_tilde = posterior_model.expectation\n",
    "    t3 = time.time()\n",
    "    print(f\"   Time: {t3-t2:.3f}s\")\n",
    "\n",
    "    # Push to property space (fast - no dense matrices involved)\n",
    "    print(\"\\n2. Computing property posterior...\")\n",
    "    t4 = time.time()\n",
    "    property_posterior = posterior_model.affine_mapping(operator=T)\n",
    "    p_tilde = property_posterior.expectation\n",
    "    t5 = time.time()\n",
    "    print(f\"   Time: {t5-t4:.3f}s\")\n",
    "\n",
    "    # Extract only the small property covariance matrix\n",
    "    print(\"\\n3. Extracting property covariance...\")\n",
    "    t6 = time.time()\n",
    "    cov_P_matrix = property_posterior.covariance.matrix(dense=True, parallel=True, n_jobs=8)\n",
    "    t7 = time.time()\n",
    "    print(f\"   Time: {t7-t6:.3f}s\")\n",
    "\n",
    "    total_time = t7 - t0\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"WORKFLOW 2 TOTAL TIME: {total_time:.3f}s\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(\"\\nBreakdown:\")\n",
    "    print(f\"  Setup:             {(t1-t0)/total_time*100:5.1f}%  ({t1-t0:.3f}s)\")\n",
    "    print(f\"  Model posterior:   {(t3-t2)/total_time*100:5.1f}%  ({t3-t2:.3f}s)\")\n",
    "    print(f\"  Push to property:  {(t5-t4)/total_time*100:5.1f}%  ({t5-t4:.3f}s)\")\n",
    "    print(f\"  Property cov:      {(t7-t6)/total_time*100:5.1f}%  ({t7-t6:.3f}s)\")\n",
    "    print(\"\\nüí° Skipped expensive dense model covariance extraction!\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# Data fit (works for both workflows)\n",
    "data_misfit = np.linalg.norm(G(m_tilde) - d_tilde)\n",
    "print(f\"\\nData misfit (posterior): {data_misfit:.4f}\")\n",
    "print(f\"Data misfit (prior):     {np.linalg.norm(G([m_0_vp, m_0_vs]) - d_tilde):.4f}\")\n",
    "\n",
    "# Show improvement\n",
    "relative_improvement = 1 - data_misfit / np.linalg.norm(G([m_0_vp, m_0_vs]) - d_tilde)\n",
    "print(f\"Data fit improvement: {100 * relative_improvement:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabca409",
   "metadata": {},
   "outputs": [],
   "source": [
    "if COMPUTE_MODEL_POSTERIOR:\n",
    "    # Visualize posterior measure on model space with samples (vp & vs)\n",
    "    fig_title = \"Model Posterior Distribution (vp & vs - Sobolev)\"\n",
    "    filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "    # Plot posterior samples\n",
    "    num_samples = 20\n",
    "    sample_color_vp = 'tab:blue'\n",
    "    sample_color_vs = 'tab:orange'\n",
    "\n",
    "    # Create two vertically stacked subplots\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 10), dpi=200, sharex=True)\n",
    "\n",
    "    print(\"Sampling from model posterior...\")\n",
    "    for i in range(num_samples):\n",
    "        sample = mu_M.sample()\n",
    "        sample_vp, sample_vs = sample\n",
    "\n",
    "        # Top subplot: vp\n",
    "        axs[0].plot(x, sample_vp.evaluate(x), color=sample_color_vp, alpha=0.25, linewidth=1,\n",
    "                    label='Posterior Samples' if i == 0 else \"\")\n",
    "\n",
    "        # Bottom subplot: vs\n",
    "        axs[1].plot(x, sample_vs.evaluate(x), color=sample_color_vs, alpha=0.25, linewidth=1,\n",
    "                    label='Posterior Samples' if i == 0 else \"\")\n",
    "\n",
    "    # Unpack direct-sum components\n",
    "    m_tilde_vp, m_tilde_vs = m_tilde\n",
    "    m_bar_vp_local, m_bar_vs_local = m_bar\n",
    "    m_prior_mean_vp, m_prior_mean_vs = M_prior.expectation\n",
    "\n",
    "    # Top subplot: vp component\n",
    "    axs[0].plot(x, m_tilde_vp.evaluate(x), color='tab:blue', linewidth=3,\n",
    "                label='Posterior Mean', zorder=10)\n",
    "    axs[0].plot(x, m_bar_vp_local.evaluate(x), color='tab:red', linestyle='--', linewidth=3,\n",
    "                label='True Model', zorder=10)\n",
    "    axs[0].plot(x, m_prior_mean_vp.evaluate(x), color='tab:green',\n",
    "                linestyle=':', linewidth=2, alpha=0.8, label='Prior Mean', zorder=5)\n",
    "\n",
    "    axs[0].set_title(r\"Model Posterior: $v_p$ component (Sobolev)\", fontsize=16)\n",
    "    axs[0].set_ylabel(\"Model Value\", fontsize=14)\n",
    "    axs[0].legend(fontsize=11, loc='upper right')\n",
    "    axs[0].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "    # Bottom subplot: vs component\n",
    "    axs[1].plot(x, m_tilde_vs.evaluate(x), color='tab:blue', linewidth=3,\n",
    "                label='Posterior Mean', zorder=10)\n",
    "    axs[1].plot(x, m_bar_vs_local.evaluate(x), color='tab:red', linestyle='--', linewidth=3,\n",
    "                label='True Model', zorder=10)\n",
    "    axs[1].plot(x, m_prior_mean_vs.evaluate(x), color='tab:green',\n",
    "                linestyle=':', linewidth=2, alpha=0.8, label='Prior Mean', zorder=5)\n",
    "\n",
    "    axs[1].set_title(r\"Model Posterior: $v_s$ component (Sobolev)\", fontsize=16)\n",
    "    axs[1].set_xlabel(r\"$x$\", fontsize=14)\n",
    "    axs[1].set_ylabel(\"Model Value\", fontsize=14)\n",
    "    axs[1].legend(fontsize=11, loc='upper right')\n",
    "    axs[1].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute reconstruction error for each component\n",
    "    reconstruction_error_vp = np.sqrt(np.mean((m_tilde_vp.evaluate(x) - m_bar_vp_local.evaluate(x))**2))\n",
    "    reconstruction_error_vs = np.sqrt(np.mean((m_tilde_vs.evaluate(x) - m_bar_vs_local.evaluate(x))**2))\n",
    "    print(f\"RMS reconstruction error (vp): {reconstruction_error_vp:.4f}\")\n",
    "    print(f\"RMS reconstruction error (vs): {reconstruction_error_vs:.4f}\")\n",
    "    print(f\"Relative error (vp): {100 * reconstruction_error_vp / np.std(m_bar_vp_local.evaluate(x)):.1f}%\")\n",
    "    print(f\"Relative error (vs): {100 * reconstruction_error_vs / np.std(m_bar_vs_local.evaluate(x)):.1f}%\")\n",
    "\n",
    "else:\n",
    "    # Visualize posterior mean only (no samples, no dense covariance)\n",
    "    fig_title = \"Model Posterior Mean (vp & vs - Sobolev)\"\n",
    "    filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "    # Create two vertically stacked subplots\n",
    "    fig, axs = plt.subplots(2, 1, figsize=(12, 10), dpi=200, sharex=True)\n",
    "\n",
    "    # Unpack direct-sum components\n",
    "    m_tilde_vp, m_tilde_vs = m_tilde\n",
    "    m_bar_vp_local, m_bar_vs_local = m_bar\n",
    "    m_prior_mean_vp, m_prior_mean_vs = M_prior.expectation\n",
    "\n",
    "    # Top subplot: vp component\n",
    "    axs[0].plot(x, m_tilde_vp.evaluate(x), color='tab:blue', linewidth=3,\n",
    "                label='Posterior Mean', zorder=10)\n",
    "    axs[0].plot(x, m_bar_vp_local.evaluate(x), color='tab:red', linestyle='--', linewidth=3,\n",
    "                label='True Model', zorder=10)\n",
    "    axs[0].plot(x, m_prior_mean_vp.evaluate(x), color='tab:green',\n",
    "                linestyle=':', linewidth=2, alpha=0.8, label='Prior Mean', zorder=5)\n",
    "\n",
    "    axs[0].set_title(r\"Model Posterior Mean: $v_p$ component (Sobolev, Fast Workflow)\", fontsize=16)\n",
    "    axs[0].set_ylabel(\"Model Value\", fontsize=14)\n",
    "    axs[0].legend(fontsize=11, loc='upper right')\n",
    "    axs[0].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "    # Bottom subplot: vs component\n",
    "    axs[1].plot(x, m_tilde_vs.evaluate(x), color='tab:blue', linewidth=3,\n",
    "                label='Posterior Mean', zorder=10)\n",
    "    axs[1].plot(x, m_bar_vs_local.evaluate(x), color='tab:red', linestyle='--', linewidth=3,\n",
    "                label='True Model', zorder=10)\n",
    "    axs[1].plot(x, m_prior_mean_vs.evaluate(x), color='tab:green',\n",
    "                linestyle=':', linewidth=2, alpha=0.8, label='Prior Mean', zorder=5)\n",
    "\n",
    "    axs[1].set_title(r\"Model Posterior Mean: $v_s$ component (Sobolev, Fast Workflow)\", fontsize=16)\n",
    "    axs[1].set_xlabel(r\"$x$\", fontsize=14)\n",
    "    axs[1].set_ylabel(\"Model Value\", fontsize=14)\n",
    "    axs[1].legend(fontsize=11, loc='upper right')\n",
    "    axs[1].grid(True, linestyle=':', alpha=0.4)\n",
    "\n",
    "    sns.despine()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Compute reconstruction error for each component\n",
    "    reconstruction_error_vp = np.sqrt(np.mean((m_tilde_vp.evaluate(x) - m_bar_vp_local.evaluate(x))**2))\n",
    "    reconstruction_error_vs = np.sqrt(np.mean((m_tilde_vs.evaluate(x) - m_bar_vs_local.evaluate(x))**2))\n",
    "    print(f\"RMS reconstruction error (vp): {reconstruction_error_vp:.4f}\")\n",
    "    print(f\"RMS reconstruction error (vs): {reconstruction_error_vs:.4f}\")\n",
    "    print(f\"Relative error (vp): {100 * reconstruction_error_vp / np.std(m_bar_vp_local.evaluate(x)):.1f}%\")\n",
    "    print(f\"Relative error (vs): {100 * reconstruction_error_vs / np.std(m_bar_vs_local.evaluate(x)):.1f}%\")\n",
    "    print(\"\\nüí° Note: Skipped sampling (no dense model covariance computed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7962514d",
   "metadata": {},
   "source": [
    "### Property Posterior \n",
    "\n",
    "We can directly compute the property posterior without explicitly constructing the full model posterior:\n",
    "$$\\mu_{\\mathcal{P}}^{\\mathbf{\\tilde{d}}} = \\mathcal{N}(\\mathbf{\\tilde{p}}, \\mathbf{C}_{\\mathcal{P}})$$\n",
    "\n",
    "where:\n",
    "- **Property mean**: $\\mathbf{\\tilde{p}} = \\mathcal{T}(\\tilde{m})$ \n",
    "- **Property covariance**: $\\mathbf{C}_{\\mathcal{P}} = \\mathcal{T} C_{\\mathcal{M}} \\mathcal{T}^*$\n",
    "\n",
    "This represents our final inferences about the local properties of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bdc7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Property posterior already computed in main inference cell\n",
    "# Just verify variables exist\n",
    "\n",
    "if 'p_tilde' not in locals() or 'cov_P_matrix' not in locals():\n",
    "    raise RuntimeError(\"Property posterior not computed - run inference cell above first\")\n",
    "\n",
    "print(f\"‚úì Property posterior available\")\n",
    "print(f\"  Property mean computed for {len(p_tilde)} locations\")\n",
    "print(f\"  Property covariance matrix size: {cov_P_matrix.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9c38cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sampling-capable property posterior measure\n",
    "gaussian_P = GaussianMeasure.from_covariance_matrix(P, cov_P_matrix, expectation=p_tilde)\n",
    "print(\"Property posterior Gaussian measure created successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8daf2020",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final property inference results\n",
    "std_P_post = np.sqrt(np.diag(cov_P_matrix))\n",
    "\n",
    "fig_title = \"Property Inference Results (Sobolev)\"\n",
    "filename = os.path.join(figures_folder, fig_title.replace(\" \", \"_\").lower() + \".png\")\n",
    "\n",
    "plt.figure(figsize=(12, 6), dpi=200)\n",
    "\n",
    "# Property posterior with uncertainty\n",
    "plt.errorbar(centers, p_tilde, yerr=2*std_P_post, fmt='o', color='tab:blue',\n",
    "            alpha=0.8, capsize=4, capthick=2, markersize=8, linewidth=2,\n",
    "            label='Posterior Properties (¬±2œÉ)')\n",
    "plt.fill_between(centers, p_tilde - 2*std_P_post, p_tilde + 2*std_P_post,\n",
    "                color='tab:blue', alpha=0.2)\n",
    "\n",
    "# True properties\n",
    "true_props = T(m_bar)\n",
    "plt.scatter(centers, true_props, label='True Properties',\n",
    "           color='tab:red', marker='x', s=120, alpha=0.9, linewidths=4, zorder=10)\n",
    "\n",
    "# Prior for comparison\n",
    "mean_prop_prior = T(M_prior.expectation)\n",
    "plt.plot(centers, mean_prop_prior, 'o--', color='tab:green', alpha=0.6,\n",
    "        markersize=6, linewidth=2, label='Prior Properties')\n",
    "\n",
    "# Sample from posterior (only if we have full covariance)\n",
    "if COMPUTE_MODEL_POSTERIOR:\n",
    "    sampled_props_post = gaussian_P.sample()\n",
    "    plt.scatter(centers, sampled_props_post, label='Posterior Sample',\n",
    "               color='purple', marker='s', s=60, alpha=0.7, zorder=5)\n",
    "\n",
    "workflow_label = \"Workflow 1\" if COMPUTE_MODEL_POSTERIOR else \"Workflow 2 (Fast)\"\n",
    "plt.xlabel('Target Location', fontsize=16)\n",
    "plt.ylabel('Property Value', fontsize=16)\n",
    "plt.title(f'Property Inference Results - Sobolev Spaces ({workflow_label})', fontsize=18)\n",
    "plt.legend(fontsize=14, loc='best')\n",
    "plt.grid(True, linestyle=':', alpha=0.4)\n",
    "sns.despine()\n",
    "plt.tight_layout()\n",
    "plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "plt.savefig(filename.replace('.png', '.pdf'), bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Quantitative assessment\n",
    "property_errors = np.abs(p_tilde - true_props)\n",
    "within_2sigma = np.sum(property_errors <= 2*std_P_post)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Workflow: {'1 (via model posterior)' if COMPUTE_MODEL_POSTERIOR else '2 (direct - fast)'}\")\n",
    "print(f\"Properties successfully inferred: {within_2sigma}/{len(true_props)} ({100*within_2sigma/len(true_props):.1f}%)\")\n",
    "print(f\"Mean absolute error: {np.mean(property_errors):.4f}\")\n",
    "print(f\"RMS error: {np.sqrt(np.mean(property_errors**2)):.4f}\")\n",
    "print(f\"Max error: {np.max(property_errors):.4f}\")\n",
    "print(f\"Average uncertainty reduction: {100*(1 - np.mean(std_P_post)/np.mean(std_P)):.1f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fdbb9ce",
   "metadata": {},
   "source": [
    "## Performance Notes\n",
    "\n",
    "### Two Workflows Compared\n",
    "\n",
    "This notebook supports two computational workflows controlled by `COMPUTE_MODEL_POSTERIOR`:\n",
    "\n",
    "#### Workflow 1: Model Posterior ‚Üí Property Posterior (Slower but Complete)\n",
    "Set `COMPUTE_MODEL_POSTERIOR = True`\n",
    "\n",
    "**Steps:**\n",
    "1. Compute model posterior measure\n",
    "2. Extract dense 2N√ó2N model covariance matrix (‚ö†Ô∏è expensive for direct-sum spaces!)\n",
    "3. Create sampling-capable model measure\n",
    "4. Push to property space via $\\mathcal{T}$\n",
    "5. Extract property covariance\n",
    "\n",
    "**Advantages:**\n",
    "- Can sample from model posterior (both vp and vs components)\n",
    "- Full model posterior visualization with uncertainty bands\n",
    "\n",
    "**Disadvantages:**\n",
    "- Slow: ~80-120 seconds for typical parameters\n",
    "- Memory intensive: O(4N¬≤) dense matrix for direct-sum space\n",
    "\n",
    "#### Workflow 2: Property Posterior Directly (Faster, Properties Only)\n",
    "Set `COMPUTE_MODEL_POSTERIOR = False`\n",
    "\n",
    "**Steps:**\n",
    "1. Compute property posterior measure directly\n",
    "2. Extract small N_p√óN_p property covariance (fast!)\n",
    "3. Compute model posterior mean only (no covariance)\n",
    "\n",
    "**Advantages:**\n",
    "- Fast: ~20-30 seconds for typical parameters\n",
    "- Memory efficient: only O(N_p¬≤) where N_p << 2N\n",
    "- Identical property posterior results\n",
    "\n",
    "**Disadvantages:**\n",
    "- Cannot sample from model posterior\n",
    "- Model space visualization shows mean only\n",
    "\n",
    "### Speedup Analysis\n",
    "\n",
    "For typical parameters (N~100 DOFs per component, N_p=20) with direct-sum Sobolev spaces:\n",
    "- **Workflow 1**: ~100 seconds\n",
    "- **Workflow 2**: ~25 seconds  \n",
    "- **Speedup**: ~4x\n",
    "\n",
    "The bottleneck in Workflow 1 is extracting the dense model covariance matrix for the direct sum, which requires many operator applications.\n",
    "\n",
    "### Recommendation\n",
    "\n",
    "**Use Workflow 2** (`COMPUTE_MODEL_POSTERIOR = False`) when:\n",
    "- You only care about property inference (most applications!)\n",
    "- Speed matters\n",
    "- Memory is limited\n",
    "\n",
    "**Use Workflow 1** (`COMPUTE_MODEL_POSTERIOR = True`) when:\n",
    "- You need samples from the model posterior for both components\n",
    "- You want full model space uncertainty visualization\n",
    "- You're computing multiple different property posteriors from the same model posterior"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
