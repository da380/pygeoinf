{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20adc560",
   "metadata": {},
   "source": [
    "# Data assimilation 2\n",
    "\n",
    "Within this lecture, we continue our discussion of data assimilation. Last time, we saw how Bayesian methods can be used to \n",
    "provide a complete solution of the data assimilation problem for deterministic and finite-dimensional dynamical systems. \n",
    "The issue, however, is that the computational cost of the full method is too high to be applied directly within \n",
    "realistic applications. Our focus in this lecture is on approximate forms for the full Bayesian solution that are computationally tractable\n",
    "and so can be applied to weather forecasting and other climatological applications.\n",
    "\n",
    "### Review of the Bayesian approach\n",
    "\n",
    "We consider a dynamical system on an $m$-dimensional state space, $\\mathbb{R}^{m}$, and write \n",
    "$\\mathbf{x}$ for a state vector. The dynamics is governed by an evolution equation of the form\n",
    "$$\n",
    "\\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t} = \\mathbf{f}(\\mathbf{x}), \n",
    "$$\n",
    "with $\\mathbf{f}(\\mathbf{x})$ known as the dynamical rule. Given an initial condition, \n",
    "$\\mathbf{x}(0) = \\mathbf{x}_{0}$, this equation has a unique solution for all times. \n",
    "We write $\\Phi$ for the flow of the system, this being the mapping such that \n",
    "$\\Phi(\\mathbf{x}_{0},t)$ is the state at time, $t$, associated with the initial condition, $\\mathbf{x}_{0}$.\n",
    "For convenience we write \n",
    "$$\n",
    "\\Phi_{t}(\\mathbf{x}_{0}) = \\Phi(\\mathbf{x},t)\n",
    "$$\n",
    "for the induced mapping on the state space at time, $t$, which \n",
    "has the inverse \n",
    "$$\n",
    "\\Phi_{t}^{-1}(\\mathbf{x}) = \\Phi(\\mathbf{x},-t).\n",
    "$$\n",
    "\n",
    "Suppose that at a set of times, $t_{1}, \\dots, t_{n}$, we have noisy partial observations\n",
    "$$\n",
    "\\mathbf{y}_{i} = \\mathbf{g}_{i}[\\mathbf{x}(t)] + \\mathbf{z}_{i}, \n",
    "$$\n",
    "where the $\\mathbf{g}_{i}:\\mathbf{R}^{m}\\rightarrow \\mathbb{R}^{n_{i}}$ are observation functions that map to lower-dimensional observation spaces,\n",
    "and $\\mathbf{z}_{i}$ are observational errors drawn from known error distributions. Our aim is to forecast the future \n",
    "state of the system for $t > t_{n}$. \n",
    "\n",
    "Within the Bayesian framework, we start with a prior probability distribution, $\\pi_{0}$, on the initial \n",
    "state with PDF, $p_{0}$. To construct our forecast we follow \n",
    "a  **prediction-analysis loop**:\n",
    "\n",
    "At the $i$-th **prediction** step, we assume that our knowledge of the state at time, $t_{i-1}$, is expressed through a distribution, $\\tilde{\\pi}_{i-1}$. We push forward this distribution under the dynamics to obtain the prior distribution \n",
    "$$\n",
    "\\pi_{i} = \\tilde{\\pi}_{i-1} \\circ \\Phi_{t_{i}-t_{i-1}}^{-1}, \n",
    "$$\n",
    "for the state at time, $t_{i}$. \n",
    "\n",
    "The $i$-th **analysis** step then applies Bayes theorem to update the prior, $\\pi_{i}$, in light of the \n",
    "observations available at this time. In terms of the associated PDFs, this update takes the form\n",
    "$$\n",
    "\\tilde{p}_{i}(\\mathbf{x})  = \\frac{q_{i}[\\mathbf{y}_{i} - \\mathbf{g}_{i}(\\mathbf{x})] p_{i}(\\mathbf{x})}{\\int_{\\mathbb{R}^{m}} q_{i}[\\mathbf{y}_{i} - \\mathbf{g}_{i}(\\mathbf{x})] p_{i}(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x}}, \n",
    "$$\n",
    "where $p_{i}$ is the prior PDF, $\\tilde{p}_{i}$ the posterior PDF, and $q_{i}$ the PDF for the corresponding error distribution.\n",
    "\n",
    "The whole process is started with a prior distribution, $\\pi_{0}$, on the initial state, and once we have assimilated all  observations\n",
    "we arrive at a posterior distribution, $\\tilde{\\pi}_{n}$, at the final observation time. Our forecast\n",
    "is then found by pushing forward $\\tilde{\\pi}_{n}$ to later times.\n",
    "\n",
    "We can also perform a **reanalysis** in which  $\\tilde{\\pi}_{n}$ is pushed back to the initial time\n",
    "to arrive at a posterior distribution, $\\tilde{\\pi}_{0}$, on the initial state. Using $\\tilde{\\pi}_{0}$, \n",
    "the state of the system over the observation period can be reconstructed if desired.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5b0b43",
   "metadata": {},
   "source": [
    "## Exact methods for a class of linear problems\n",
    "\n",
    "The aim of this section is to show that in important special case, the full Bayesian solution of the data\n",
    "assimilation problem can be obtained in a simpler manner. This special case  forms the basis for the\n",
    " approximate methods discussed below. \n",
    "\n",
    "### The flow of a linear dynamical system\n",
    "\n",
    "Recall that a dynamical system is linear if its evolution equation takes the form\n",
    "$$\n",
    "\\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t} = \\mathbf{A}\\mathbf{x}, \n",
    "$$\n",
    "with $\\mathbf{A}$ a matrix that maps the state space to itself. The solution of this equation subject to \n",
    "the initial condition $\\mathbf{x}(0) = \\mathbf{x}_{0}$ can be written\n",
    "$$\n",
    "\\mathbf{x}(t) = \\exp(\\mathbf{A}t)\\mathbf{x}_{0}, \n",
    "$$\n",
    "where the exponential of a matrix is defined through its Taylor expansion\n",
    "$$\n",
    "\\exp(\\mathbf{A}) = \\sum_{j=0}^{\\infty} \\frac{1}{j!}\\mathbf{A}^{j}.\n",
    "$$\n",
    "The validity of the above solution can be verfied by differentiating the appropriate Taylor\n",
    "series term by term.  It follows that the flow of a linear dynamical system takes the form\n",
    "$$\n",
    "\\Phi(\\mathbf{x}_{0},t) = \\exp( \\mathbf{A} t)\\mathbf{x}_{0}, \n",
    "$$\n",
    "and hence the induced mapping, $\\Phi_{t}$, is linear, abd corresponds to the matrix, $\\exp( \\mathbf{A} t)$.\n",
    "\n",
    "### Linearisation of a dynamical system about equilibrium\n",
    "\n",
    "While some physical systems are linear, it is more usual for linear evolution equations to arise as \n",
    "an approximation to the full dynamics. \n",
    "\n",
    "An **equilibrium state**, $\\mathbf{x}_{0}$, of a dynamical system is defined through the condition\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}_{0}) = \\mathbf{0}.\n",
    "$$\n",
    "It is clear that if a system is in an equilibrium state at time, $t=0$, then it is in this state for all other times. \n",
    "With $\\mathbf{x}_{0}$ an equilibrium state, consider the initial condition, $\\mathbf{x}_{0} + \\Delta\\mathbf{x}_{0}$, with \n",
    "the perturbation term, $ \\Delta\\mathbf{x}_{0}$, suitably small. At a general time, $t$, we can then write the state as\n",
    "$$\n",
    "\\mathbf{x}(t) = \\mathbf{x}_{0} + \\Delta \\mathbf{x}(t) + o(\\|\\Delta\\mathbf{x}_{0}\\|).\n",
    "$$\n",
    "Putting this expansion into the evolution equation and retaining only first-order terms we obtain\n",
    "$$\n",
    "\\frac{\\mathrm{d}\\Delta \\mathbf{x}}{\\mathrm{d}t} =  \\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}(\\mathbf{x}_{0}) \\Delta \\mathbf{x}, \n",
    "$$\n",
    "which is subject to the initial condition, $\\Delta\\mathbf{x}(0) = \\Delta \\mathbf{x}_{0}$. This is a linear \n",
    "dynamical system for the perturbation about the equilibrium state. For sufficiently small initial \n",
    "perturbations, the linearised system should approximate well the system's full dynamics. Recalling the definition of the sensitivity matrix from last lecture, we note that\n",
    "$$\n",
    "\\Delta\\mathbf{x}(t) = \\frac{\\partial \\Phi_{t}}{\\partial \\mathbf{x}_{0}}(\\mathbf{x}_{0})\\Delta \\mathbf{x}_{0}.\n",
    "$$\n",
    "\n",
    "### Application to the pendulum system\n",
    "\n",
    "These ideas are readily applied to the pendulum system which has dynamical rule\n",
    "$$\n",
    "\\mathbf{f}(\\mathbf{x}) = \\left(\\begin{array}{c}\n",
    "              \\frac{1}{ml^{2}}p \\\\ - mgl \\sin \\theta\n",
    "             \\end{array}\\right), \n",
    "$$\n",
    "and hence\n",
    "$$\n",
    "\\frac{\\partial \\mathbf{f}}{\\partial \\mathbf{x}}[\\mathbf{x}(t)] = \\left(\\begin{array}{cc}\n",
    "              0 && \\frac{1}{ml^{2}} \\\\ - mgl \\cos \\theta && 0\n",
    "             \\end{array}\\right).\n",
    "$$\n",
    "By inspection, an equilibrium state is\n",
    "$$\n",
    "\\mathbf{x}_{0} = \\left(\\begin{array}{c}\n",
    "             0 \\\\ 0\n",
    "             \\end{array}\\right), \n",
    "$$\n",
    "and the corresponding linearised evolution equation is\n",
    "$$\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d}t }\\left(\\begin{array}{c}\n",
    "             \\Delta \\theta \\\\ \\Delta p\n",
    "             \\end{array}\\right) = \\left(\\begin{array}{cc}\n",
    "              0 && \\frac{1}{ml^{2}} \\\\ - mgl  && 0\n",
    "             \\end{array}\\right) \\left(\\begin{array}{c}\n",
    "             \\Delta \\theta \\\\ \\Delta p\n",
    "             \\end{array}\\right).\n",
    "$$\n",
    "Many of you will recognise this as being an instance of the equations for **simple harmonic motion**. \n",
    "For the pendulum system, there is a further equilbrium point at \n",
    "$$\n",
    "\\mathbf{x}_{0} = \\left(\\begin{array}{c}\n",
    "             \\pi \\\\ 0\n",
    "             \\end{array}\\right). \n",
    "$$\n",
    "It is clear physically that the first of these equilbrium states is stable, while the latter is not. The idea of stablility can be formalised, \n",
    "but we simply note that linearised approximations to the dynamics are only useful about a stable equilibrium. \n",
    "\n",
    "Within this [file](../data_assimilation/pendulum/single/physics.py) the dynamical rule for the and linearised pendulum system have been implemented, and in the code below we use then to investigate the accuracy of the linearisation for different initial perturbations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d62839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries for this notebook, \n",
    "# installing pygeoinf if required. \n",
    "try:\n",
    "    from pygeoinf import data_assimilation as da\n",
    "except ImportError: \n",
    "    %pip install pygeoinf --quiet\n",
    "    from pygeoinf import data_assimilation as da\n",
    "\n",
    "from pygeoinf.data_assimilation.pendulum import single\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "\n",
    "# Setup Parameters\n",
    "t_points = np.linspace(0, 50, 500)\n",
    "\n",
    "# Set the initial perturbation\n",
    "theta_0 = 10\n",
    "y0 = [np.deg2rad(theta_0), 0.0] \n",
    "\n",
    "# True Non-linear physics\n",
    "sol_nl = da.solve_trajectory(\n",
    "    single.physics.eom,\n",
    "    y0, \n",
    "    t_points\n",
    ")\n",
    "\n",
    "# Linearization at equilibrium\n",
    "sol_l = da.solve_trajectory(\n",
    "    single.physics.eom_linear, \n",
    "    y0, \n",
    "    t_points\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8), sharex=True)\n",
    "\n",
    "ax1.plot(t_points, sol_nl[0], 'k-', lw=2, label='Non-linear (True)')\n",
    "ax1.plot(t_points, sol_l[0], 'r--', lw=2, label='Linear (Approx)')\n",
    "ax1.set_ylabel(r\"$\\theta$\")\n",
    "ax1.set_title(\"Evolution of Angle\")\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(t_points, sol_nl[1], 'k-', lw=2, label='Non-linear (True)')\n",
    "ax2.plot(t_points, sol_l[1], 'r--', lw=2, label='Linear (Approx)')\n",
    "ax2.set_ylabel(r\"$p$\")\n",
    "ax2.set_xlabel(\"Time\")\n",
    "ax2.set_title(\"Evolution of Momentum\")\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle(f'Accuracy of Linearization (Initial $\\\\theta_{{0}} = {theta_0}^\\\\circ$)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e98bec",
   "metadata": {},
   "source": [
    "\n",
    "### Multivariate Gaussian distributions\n",
    "\n",
    "A Gaussian distribution on $\\mathbb{R}^{m}$ has PDF of the form\n",
    "$$\n",
    "p(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^{m} \\det\\mathbf{Q}}}\n",
    "\\mathrm{exp}\\!\\left[\n",
    "    -\\frac{1}{2}(\\mathbf{x}-\\overline{\\mathbf{x}})^{T}\\mathbf{Q}^{-1}(\\mathbf{x}-\\overline{\\mathbf{x}})\n",
    "    \\right], \n",
    "$$\n",
    "where $\\overline{\\mathbf{x}}$ is the **mean** value and $\\mathbf{Q}$ is the **covariance**. The latter matrix needs to be symmetric and invertible\n",
    "(Gaussian distributions with singular covariances can be defined but we do not need to consider this case). \n",
    "\n",
    "For a general PDF on $\\mathbb{R}^{m}$, its **mean** is defined by \n",
    "$$\n",
    "\\overline{\\mathbf{x}} = \\int_{\\mathbb{R}^{m}} \\mathbf{x} \\,p(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x}, \n",
    "$$\n",
    "while its **covariance** is given by \n",
    "$$\n",
    "\\mathbf{Q} = \\int_{\\mathbb{R}^{m}} (\\mathbf{x}-\\overline{\\mathbf{x}}) (\\mathbf{x}-\\overline{\\mathbf{x}})^{T} p(\\mathbf{x}) \\,\\mathrm{d}\\mathbf{x}. \n",
    "$$\n",
    "The mean and covariance calculated for a Gaussian distribution are precisely the parameters occurring within its PDF, and hence our terminology is consistent. \n",
    "\n",
    "In what follows, we write $\\mathcal{N}(\\overline{\\mathbf{x}},\\mathbf{Q})$ for the  Gaussian distribution with mean, $\\overline{\\mathbf{x}}$ and covariance, $\\mathbf{Q}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b03be",
   "metadata": {},
   "source": [
    "\n",
    "### Push forward of a Gaussian under a linear flow\n",
    "\n",
    "Recall from the last lecture that a distribtion $\\pi$ with PDF, $p$, is transformed to \n",
    "$$\n",
    "p_{t}(\\mathbf{x}) = p_{0}[\\Phi_{t}^{-1}(\\mathbf{x})]  \\det\\!\\left\\{\n",
    "    \\frac{\\partial \\Phi_{t}}{\\partial \\mathbf{x}_{0}}[\\Phi_{t}^{-1}(\\mathbf{x})]\\right\\}^{-1}, \n",
    "$$\n",
    "under the flow of a dynamical system. When the dynamical system is linear and \n",
    "the initial distribution is Gaussian, the push forward of the distribution is also Gaussian and there\n",
    "exist simple relationships between the means and covariances. \n",
    "\n",
    "For a linear dynamical system, we know that $\\Phi_{t}(\\mathbf{x}_{0}) = \\exp(\\mathbf{A}t)\\mathbf{x}_{0}$, and hence\n",
    "$$\n",
    "\\frac{\\partial \\Phi_{t}}{\\partial \\mathbf{x}_{0}}[\\Phi_{t}^{-1}(\\mathbf{x})] = \\exp(\\mathbf{A} t).\n",
    "$$\n",
    " Applying the general transformation formula, it follows that the Gaussian distribution, $\\mathcal{N}(\\overline{\\mathbf{x}},\\mathbf{Q})$,\n",
    " becomes \n",
    "$$\n",
    "p_{t}(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^{m} \\det\\mathbf{Q}}}\n",
    "\\mathrm{exp}\\!\\left[\n",
    "    -\\frac{1}{2}(\\mathrm{e}^{-\\mathbf{A}t}\\mathbf{x}-\\overline{\\mathbf{x}})^{T}\\mathbf{Q}^{-1}\n",
    "    (\\mathrm{e}^{-\\mathbf{A}t}\\mathbf{x}-\\overline{\\mathbf{x}})\n",
    "    \\right]  \\frac{1}{\\det \\exp(\\mathbf{A} t)}.\n",
    "$$\n",
    "Through a simple rearrangement, this can be written\n",
    "$$\n",
    "p_{t}(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^{m} \\det\\mathbf{Q}_{t}}}\n",
    "\\mathrm{exp}\\!\\left[\n",
    "    -\\frac{1}{2}(\\mathbf{x}-\\overline{\\mathbf{x}}_{t})^{T}\\mathbf{Q}_{t}^{-1}\n",
    "    (\\mathbf{x}-\\overline{\\mathbf{x}}_{t})\n",
    "    \\right], \n",
    "$$\n",
    "where we have defined \n",
    "$$\n",
    "\\overline{\\mathbf{x}}_{t} = \\exp(\\mathbf{A} t)\\overline{\\mathbf{x}}, \\quad \n",
    "\\mathbf{Q}_{t} = \\exp(\\mathbf{A} t)\\mathbf{Q} \\exp(\\mathbf{A} t)^{T}.\n",
    "$$\n",
    "Thus, the transformed distribution is still Gaussian, and its mean and covariance \n",
    "can be readily determined. \n",
    "\n",
    "The code below illustrates this idea. First, we perform the transformation of a Gaussian PDF using the general method from the last lecture, but \n",
    "applied to the linearised pendulum equations. Next, we transform the mean and covariance of the initial distribution using the above formulae. The \n",
    "two PDFs are plotted side by side and we can see that they look identical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331e3eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Parameters and Initial Correlated Prior\n",
    "t = 25.0\n",
    "mean_0 = np.array([0.2, 0.0]) \n",
    "cov_0 = np.array([[0.05, 0.01], [0.01, 0.05]]) \n",
    "\n",
    "# Create the prior on a grid\n",
    "theta_range = (-np.pi, np.pi)\n",
    "p_range = (-3, 3)\n",
    "resolution = 300\n",
    "\n",
    "prior_0 = da.ProbabilityGrid.from_bounds(\n",
    "    (theta_range, p_range), \n",
    "    resolution, \n",
    "    da.get_gaussian_pdf(mean_0, cov_0)\n",
    ")\n",
    "\n",
    "\n",
    "# Push forward the prior under the linearised dynamics\n",
    "prior_1 = prior_0.push_forward(single.eom_linear, t)\n",
    "\n",
    "\n",
    "# Map the mean and covariance analytically\n",
    "P = single.physics.get_linear_propagator(t)\n",
    "mean_t = P @ mean_0 \n",
    "cov_t = P @ cov_0 @ P.T \n",
    "prior_1_analytical = da.ProbabilityGrid.from_bounds(\n",
    "    (theta_range, p_range), \n",
    "    resolution, \n",
    "    da.get_gaussian_pdf(mean_t, cov_t)\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(11, 5), sharex=True, sharey=True)\n",
    "titles = [\"Numerical push forward\", \"Analytical push forward)\"]\n",
    "\n",
    "priors = [prior_1, prior_1_analytical]\n",
    "\n",
    "for prior, ax, title in zip(priors, axes, titles):    \n",
    "    da.plot_grid_marginal(prior, ax=ax, cmap=\"Blues\")\n",
    "    ax.set_title(title)\n",
    "    ax.grid(True, color='white', alpha=0.3, zorder=10)\n",
    "    ax.set_xlim(-1.0, 1.0)\n",
    "    ax.set_ylim(-1.0, 1.0)\n",
    "    ax.set_aspect(\"equal\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc208ba",
   "metadata": {},
   "source": [
    "### Updating a Gaussian prior subject to linear observations with Gaussian noise\n",
    "\n",
    "Suppose that our knowledge of the state, $\\mathbf{x}$, is described by the Gaussian prior, \n",
    "$\\mathcal{N}(\\overline{\\mathbf{x}},\\mathbf{Q})$. We are then given a noisy partial observation \n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{G}\\mathbf{x} + \\mathbf{z}, \n",
    "$$\n",
    "where $\\mathbf{G}$ is a matrix that maps to a lower-dimensional observation space, and \n",
    "$\\mathbf{z}$ is a random error with distribution $\\mathcal{N}(\\mathbf{0}, \\mathbf{R})$;\n",
    "non-zero means for the error distribution can be readily handled if required. \n",
    "\n",
    "Using Bayes theorem, we can then determine the posterior distribution \n",
    "for $\\mathbf{x}$. In this case, the prior PDF takes the form\n",
    "$$\n",
    "p(\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^{m} \\det\\mathbf{Q}}}\n",
    "\\mathrm{exp}\\!\\left[\n",
    "    -\\frac{1}{2}(\\mathbf{x}-\\overline{\\mathbf{x}})^{T}\\mathbf{Q}^{-1}(\\mathbf{x}-\\overline{\\mathbf{x}})\n",
    "    \\right], \n",
    "$$\n",
    "while the likelihood is given by\n",
    "$$\n",
    "q(\\mathbf{y}-\\mathbf{G}\\mathbf{x}) = \\frac{1}{\\sqrt{(2\\pi)^{m} \\det\\mathbf{R}}}\n",
    "\\mathrm{exp}\\!\\left[\n",
    "    -\\frac{1}{2}(\\mathbf{y}-\\mathbf{G}\\mathbf{x})^{T}\\mathbf{R}^{-1}(\\mathbf{y}-\\mathbf{G}\\mathbf{x})\n",
    "    \\right].\n",
    "$$\n",
    "The product of these two PDFs is clearly proportional to the exponential of a quadratic function \n",
    "of $\\mathbf{x}$, with this suggesting that the posterior is also Gaussian. This is indeed true, \n",
    "and through some matrix algebra it can be shown that posterior distribution has mean\n",
    "$$\n",
    "\\tilde{\\overline{\\mathbf{x}}} = \\overline{\\mathbf{x}} + \\mathbf{Q}\\mathbf{G}^{T}(\\mathbf{G}\\mathbf{Q}\\mathbf{G}^{T} + \\mathbf{R})^{-1}\n",
    "(\\mathbf{y} - \\mathbf{G}\\overline{\\mathbf{x}}), \n",
    "$$\n",
    "and covariance\n",
    "$$\n",
    "\\tilde{\\mathbf{Q}} = \\mathbf{Q} - \\mathbf{Q}\\mathbf{G}^{T}(\\mathbf{G}\\mathbf{Q}\\mathbf{G}^{T} + \\mathbf{R})^{-1}\n",
    "\\mathbf{G}\\mathbf{Q}.\n",
    "$$\n",
    "From the form of the posterior covariance, it can be shown that an observation always leads to a gain in \n",
    "information. \n",
    "\n",
    "Within the code below, we perform an analysis step for an observation of $\\theta$ using the general method of the last lecture and the above analytical \n",
    "formulae for the posterior mean and covariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa852a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Gaussian prior\n",
    "prior_mean = np.array([0.5, 0.0])\n",
    "prior_cov  = np.array([[0.5, -0.1], \n",
    "                      [-0.1, 0.3]])\n",
    "\n",
    "# Set the observation\n",
    "y_obs = np.array([0.8])\n",
    "R     = np.array([[0.01]])\n",
    "G     = np.array([[1.0, 0.0]])  \n",
    "\n",
    "# Calcuate the posterior mean and covariance\n",
    "S = G @ prior_cov @ G.T + R\n",
    "K = prior_cov @ G.T @ np.linalg.inv(S)    \n",
    "post_mean = prior_mean + K @ (y_obs - G @ prior_mean)\n",
    "post_cov = (np.eye(2) - K @ G) @ prior_cov\n",
    "\n",
    "print(f\"Analytical Posterior Mean: {post_mean}\")\n",
    "print(f\"Analytical Posterior Cov:\\n{post_cov}\")\n",
    "\n",
    "# Set the prior on a numerical grid \n",
    "prior = da.ProbabilityGrid.from_bounds(\n",
    "    (theta_range, p_range), \n",
    "    resolution, \n",
    "    da.get_gaussian_pdf(prior_mean, prior_cov)\n",
    ")\n",
    "\n",
    "# Determine the likelihood\n",
    "likelihood = da.LinearGaussianLikelihood(\n",
    "    y_obs, \n",
    "    R, \n",
    "    G\n",
    ").evaluate(prior)\n",
    "\n",
    "# Bayesian update -- done here using a class method\n",
    "posterior, _ = prior.bayes_update(likelihood)\n",
    "\n",
    "# Visualisation\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 7), sharey=True)\n",
    "\n",
    "# Plot the prior and show the observed angle\n",
    "da.plot_grid_marginal(prior, ax=ax1, cmap=\"Blues\")\n",
    "ax1.set_title(\"Numerical posterior\")\n",
    "ax1.plot(prior_mean[0], prior_mean[1], 'k*', ms=12, label='Prior Mean')\n",
    "ax1.axvline(\n",
    "    y_obs[0], \n",
    "    color='k', \n",
    "    linestyle='--', \n",
    "    linewidth=2, \n",
    "    label=f'Obs: $\\\\theta={y_obs[0]}$'\n",
    ")\n",
    "ax1.axvspan(\n",
    "    y_obs[0] - 2*np.sqrt(R[0,0]),\n",
    "    y_obs[0] + 2*np.sqrt(R[0,0]), \n",
    "    color='green',\n",
    "    alpha=0.3, \n",
    "    label='Obs Noise ($2\\sigma$)'\n",
    ")\n",
    "ax1.legend(loc='upper right')\n",
    "\n",
    "\n",
    "# Plot the grid posterior \n",
    "da.plot_grid_marginal(posterior, ax=ax2, cmap=\"Blues\")\n",
    "ax2.set_title(\"Grid Posterior vs Analytical Solution\")\n",
    "\n",
    "# Plot the analytical result as an 2-std ellipse\n",
    "ax2.plot(\n",
    "    post_mean[0], \n",
    "    post_mean[1], \n",
    "    'r+', \n",
    "    ms=15, \n",
    "    markeredgewidth=2, \n",
    "    label='Analytical Mean'\n",
    ")\n",
    "vals, vecs = np.linalg.eig(post_cov)\n",
    "angle = np.degrees(np.arctan2(vecs[1, 0], vecs[0, 0]))\n",
    "width, height = 4 * np.sqrt(vals) \n",
    "ell = Ellipse(\n",
    "    xy=post_mean, \n",
    "    width=width, \n",
    "    height=height, \n",
    "    angle=angle, \n",
    "    edgecolor='red', \n",
    "    facecolor='none', \n",
    "    linestyle='--', \n",
    "    lw=2, \n",
    "    label='Analytical 2$\\sigma$ Covariance'\n",
    ")\n",
    "ax2.add_patch(ell)\n",
    "\n",
    "ax2.legend(loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae69bec",
   "metadata": {},
   "source": [
    "### The Kalman filter\n",
    "\n",
    "We have now assembled all the tools needed to develop Bayesian data assimilation for a linear dynamical system for which the prior is Gaussian, all observations and linear, and the error distributions are also Gaussian. Given these assumptions, all posterior distributions will also be Gaussian, and so we can specify them in terms of their means and covariances alone. \n",
    "\n",
    "Consider the $i$-th prediction step. At the previous time, $t_{i-1}$, our knowledge of the \n",
    "system is described by a distribution, $\\mathcal{N}(\\tilde{\\overline{\\mathbf{x}}}_{i-1},\\tilde{\\mathbf{Q}}_{i-1})$. \n",
    "The prior distribution, $\\mathcal{N}(\\overline{\\mathbf{x}}_{i}, \\mathbf{Q}_{i})$ at time, $t_{i}$, then \n",
    "has mean \n",
    "$$\n",
    "\\overline{\\mathbf{x}}_{i} = \\mathbf{P}_{i}\\tilde{\\overline{\\mathbf{x}}}_{i-1}\n",
    "$$\n",
    "and covariance\n",
    "$$\n",
    "\\mathbf{Q}_{i} = \\mathbf{P}_{i} \\tilde{\\mathbf{Q}}_{i-1}\\mathbf{P}_{i}^{T}, \n",
    "$$\n",
    "where we write $\\mathbf{P}_{i} = \\exp[\\mathbf{A}(t_{i}-t_{i-1})]$ for convenience. \n",
    "\n",
    "Following this, we are given at time, $t_{i}$, the partial observation\n",
    "$\\mathbf{y}_{i} = \\mathbf{G}_{i}\\mathbf{x}_{i} + \\mathbf{z}_{i}$ with error distribution\n",
    "$\\mathcal{N}(\\mathbf{0}, \\mathbf{R}_{i})$. Applying the Bayes theorem,  we compute the posterior mean\n",
    "$$\n",
    "\\tilde{\\overline{\\mathbf{x}}}_{i} = \\overline{\\mathbf{x}}_{i} + \\mathbf{K}_{i}\n",
    "(\\mathbf{y}_{i}-\\mathbf{G}_{i}\\overline{\\mathbf{x}}_{i}), \n",
    "$$\n",
    "and posterior covariance\n",
    "$$\n",
    "\\tilde{\\mathbf{Q}}_{i} = [\\mathbf{1} - \\mathbf{K}_{i}\\mathbf{G}_{i}]\\mathbf{Q}_{i}, \n",
    "$$\n",
    "where we have defined\n",
    "$$\n",
    "\\mathbf{K}_{i} = \\mathbf{Q}_{i}\\mathbf{G}_{i}^{T}(\\mathbf{G}_{i}\\mathbf{Q}_{i}\n",
    "\\mathbf{G}_{i}^{T} + \\mathbf{R}_{i})^{-1}, \n",
    "$$\n",
    "which is known as the **Kalman gain**.\n",
    "\n",
    "This process starts with a prior distribution on the initial state, $\\mathcal{N}(\\overline{\\mathbf{x}}_{0}, \n",
    "\\mathbf{Q}_{0})$, and we end with a posterior distribution, $\\mathcal{N}(\\tilde{\\overline{\\mathbf{x}}}_{n}, \n",
    "\\tilde{\\mathbf{Q}}_{n})$ at the $n$-th observation time. Forecasts are then obtained by analytically mapping  $\\mathcal{N}(\\tilde{\\overline{\\mathbf{x}}}_{n}, \n",
    "\\tilde{\\mathbf{Q}}_{n})$ to later times, while reanalysis proceeds similarly if required. \n",
    "\n",
    "The process we have described is known as the **Kalman filter** or **KF**. This is names after Rudolf Kalman who introduced the method, though it is notable that the \n",
    "original derivation was not framed using Bayesian techniques. For linear dynamical systems, the Kalman filter provides a complete solution \n",
    "of the data assimilation problem so long as its underlying assumptions on the prior distribution and observations are met. The method can also be readily applied to linearised\n",
    "dynamical systems. The results then are only approximate, but if the initial prior is sufficiently concentrated about a stable equilibrium state the results \n",
    "will be accurate. \n",
    "\n",
    "For a linear or linearised dynamical systems whose state space is not too large, say less than about 10000, it is possible to implement the Kalman filter directly on \n",
    "a decent laptop or desktop.  As the size of the state space increases, however, the cost of storing and updating the covariances matrices can become prohibative. Here one\n",
    "option is to split the calculations in parallel over multiple processors. There also exist useful approximate methods that allow the covariance matrices to be well approximated\n",
    "using low-rank factorisations that capture most of the information but require only a small fraction of the full storage cost. In the latter context, note that implementation \n",
    "of the Kalman filter does not actually require the matrix exponentials,  $\\mathbf{P}_{i} = \\exp[\\mathbf{A}(t_{i}-t_{i-1})]$, to be calculated and stored. Indeed, the action \n",
    "of $\\mathbf{P}_{i}$ on a vector simply means integrate the evolution equation over the interval $[t_{i-1},t_{i}]$  subject to given initial conditions. Similarly,  \n",
    "for the action of $\\mathbf{P}_{i}^{T}$ we  note the identity\n",
    "$$\n",
    "\\exp(\\mathbf{A}t)^{T} = \\exp(\\mathbf{A}^{T}t), \n",
    "$$\n",
    "which follows immediately from the definition of the exponential of a matrix. This means that the action of $\\mathbf{P}_{i}^{T}$ on a vector corresponds to \n",
    "integration of the so-called **adjoint evolution equation**\n",
    "$$\n",
    "\\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t} = \\mathbf{A}^{T}\\mathbf{x}, \n",
    "$$\n",
    "over $[t_{i-1},t_{i}]$. Using these ideas, it is possible to develop efficient matrix-free implementations of the Kalman filter that can be applied to very \n",
    "high-dimensional linear or linearised dynamical syetems.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c6f45a",
   "metadata": {},
   "source": [
    "## Approximate methods for non-linear problems\n",
    "\n",
    "To conclude these lectures, we will discuss in outline two popular methods that can provide workable approximate solutions to non-linear data assimilation problems in very high-dimensional problems. Within the practicals, you will implement the second of these methods yourself. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8922823",
   "metadata": {},
   "source": [
    "\n",
    "### The extended Kalman filter\n",
    "\n",
    "We consider a non-linear dynamical system with evolution equation\n",
    "$$\n",
    "\\frac{\\mathrm{d}\\mathbf{x}}{\\mathrm{d}t} = \\mathbf{f}(\\mathbf{x}). \n",
    "$$\n",
    "Recall that the sensitivity matrix for this system is defined through\n",
    "$$\n",
    "\\Phi_{t}(\\mathbf{x}_{0} + \\Delta\\mathbf{x}_{0}) - \\Phi_{t}(\\mathbf{x}_{0}) = \\frac{\\partial \\Phi_{t}}{\\partial\\mathbf{x}_{0}}(\\mathbf{x}_{0}) \\Delta\\mathbf{x}_{0} + o(\\|\\Delta \\mathbf{x}_{0}\\|), \n",
    "$$\n",
    "and that it's value can be obtained by solution of a linear matrix ODE. The sensitivity matrix allows us to approximate the dynamics of the system for states that lie sufficiently close to a reference trajectory. \n",
    "\n",
    "\n",
    "The idea behind the **Extended Kalman filter** or  **ExKF** is to modify the prediction step within the Kalman filter to account *exactly* for the non-linear dynamics within the mean update, and approximately so for the covariance using the sensitivity matrix. The formula for the mean update is then generalised to\n",
    "$$\n",
    "\\overline{\\mathbf{x}}_{i} = \\Phi_{t_{i}-t_{i-1}}(\\tilde{\\overline{\\mathbf{x}}}_{i-1}).\n",
    "$$\n",
    "For the covariance we use\n",
    "$$\n",
    "\\mathbf{Q}_{i} = \\frac{\\partial \\Phi_{t_{i}-t_{i-1}}}{\\partial\\mathbf{x}_{0}}(\\tilde{\\overline{\\mathbf{x}}}_{i-1}) \\tilde{\\mathbf{Q}}_{i-1} \\frac{\\partial \\Phi_{t_{i}-t_{i-1}}}{\\partial\\mathbf{x}_{0}}(\\tilde{\\overline{\\mathbf{x}}}_{i-1})^{T}.\n",
    "$$\n",
    "When these formulae are applied to a linear dynamical system they reduce exactly to the prediction step for the Kalman filter. Ultimately, the ExKF arises through\n",
    "a Gaussian approximation of the pushed-forward distribution at each prediction step, and hence the approximation will be a good one if the the PDF at $t_{i-1}$ \n",
    "is concentrated sufficiently closely about its mean. \n",
    "\n",
    "The analysis step of the ExKF is identical to the KF given linear observations with Gaussian errors. At this point, more general observation operators and error distributions\n",
    "might also be considered, though again the idea would again be to approximate the true posterior as a Gaussian using linearisation. \n",
    "\n",
    "As with the KF, a direct implementation of the ExKF is possible for systems whose dimensions are not too large. There is an added cost of having to derive and solve for the sensitivity matrix. This can be done directly, but it is also a situation in which techniques like **automatic differentiation** are often applied.\n",
    "\n",
    "For problems in higher dimensions, the ExKF suffers from the same memory issues as the KF. But again, efficient matrix-free formulations can be developed coupled to low-rank approximations for the covariances. In particular, the **adjoint method** hinted at above can be extended to allow for the action of the transpose of the sensitivity matrix to be expressed through a single solution of a linear ordinary differential equation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7c4f83",
   "metadata": {},
   "source": [
    "### The ensemble Kalman filter\n",
    "\n",
    "A downside of the ExKF is the need to develop and implement methods for computing the action of the sensitivity matrix and its transpose. While this is certainly possible, and methods like automatic differentiation help, this remains a rather involved process, and particularly for climate models whose governing equations can be extremely complicated. An alternative method is the **ensemble Kalman filter** or **EnKF**. \n",
    "\n",
    "In this method we begin with our prior on the initial state, $\\mathcal{N}(\\overline{\\mathbf{x}}_{0}, \\mathbf{Q}_{0})$, and draw a set of $p$ samples:\n",
    "$$\n",
    "\\{\\mathbf{x}_{0}^{1}, \\dots, \\mathbf{x}_{0}^{p}\\}.\n",
    "$$\n",
    "During the first prediction step, we evolve each state within this ensemble under the exact non-linear dynamics to obtain\n",
    "$$\n",
    "\\{\\mathbf{x}_{1}^{1}, \\dots, \\mathbf{x}_{1}^{p}\\}, \n",
    "$$\n",
    "where we have set $\\mathbf{x}_{1}^{k} = \\bm{\\Phi}_{t_{1}}(\\mathbf{x}_{0}^{k})$. From this ensemble we can determine a **sample mean**\n",
    "$$\n",
    "\\overline{x}_{1} = \\frac{1}{N} \\sum_{k=1}^{p}\\mathbf{x}_{1}^{k}, \n",
    "$$\n",
    "and the **sample covariance**\n",
    "$$\n",
    "\\mathbf{Q}_{1} = \\frac{1}{N-1} \\sum_{k=1}^{p}(\\mathbf{x}_{1}^{k}-\\overline{\\mathbf{x}}_{1})(\\mathbf{x}_{1}^{k}-\\overline{\\mathbf{x}}_{1})^{T}.\n",
    "$$\n",
    "As a detail,  the factor of $N-1$ in the definition of the sample covariance is needed for this estimator to be unbiased.\n",
    "\n",
    "If the dynamical syetem were linear, and if the number of samples in our ensemble were sufficiently great, then it can be shown that \n",
    "the sample mean and covariance tend to the true values for the underlying Gaussian distribution. In fact, even if the \n",
    "dynamics is non-linear this covergence still happens, though a general distribution cannot be fully characterised by \n",
    "these quantities alone. \n",
    "\n",
    "For the analysis step, suppose that we have observation $\\mathbf{y}_{1} = \\mathbf{G}_{1}\\mathbf{x}_{1} + \\mathbf{z}_{1}$ with \n",
    "error distribution, $\\mathcal{N}(\\mathbf{0}, \\mathbf{R}_{1})$. For the $k$-th element of the ensemble, we first \n",
    "take the observed data, $\\mathbf{y}_{1}$, and perturb in with a random sample from the error distribution\n",
    "$$\n",
    "\\tilde{\\mathbf{y}}_{1} = \\mathbf{y}_{1} + \\mathbf{z}_{1}.\n",
    "$$\n",
    "Having done this, the ensemble member is updated according to \n",
    "$$\n",
    "\\tilde{\\mathbf{x}}_{1}^{k} = \\mathbf{x}_{1}^{k} + \\mathbf{K}_{1}(\\tilde{\\mathbf{y}}_{1} - \\mathbf{G}_{1}\\mathbf{x}_{1}^{k}), \n",
    "$$\n",
    "where the Kalman gain is calculated using the sample covariance\n",
    "$$\n",
    "\\mathbf{K}_{1} = \\mathbf{Q}_{1}\\mathbf{G}_{1}^{T}(\\mathbf{G}_{1}\\mathbf{Q}_{1}\\mathbf{G}_{1}^{T}+ \\mathbf{R}_{1})^{-1}.\n",
    "$$\n",
    "The sample mean and sample covariance can then be recalculated using the updated ensemble, $\\{\\tilde{\\mathbf{x}}_{1}^{1}, \\dots,\\tilde{\\mathbf{x}}_{1}^{p} \\}$\n",
    "to summarise our knowledge of the posterior distribution at this time. \n",
    "\n",
    "As before, for a linear dynamical system, it can be shown that given\n",
    "enough samples, this process converges to the mean and covariance of the true posterior distribution, while for non-linear systems it \n",
    "converges to a Gaussian approximation of the true posterior. A key point within the analysis step is the addition of  random errors\n",
    "to the observation when updating each member of the ensemble.  Without this step, it can be shown that uncertainties \n",
    "will be significantly underestimated. \n",
    "\n",
    "This prediction-analysis loop is then repeated to assimilate all available  observations. Each step just requires the integration of the \n",
    "ensemble forward in time using the exact non-linear dynamics, coupled to an analysis step that nudges the ensemble based on the \n",
    "available data. \n",
    "\n",
    "The great virtue of the ensemble Kalman filter is that is requires nothing more than a code for integrating the exact non-linear dynamics, while \n",
    "the memory requirements are limited to storing the $p$ members of the ensemble. Indeed, within an efficient implementation it is not necessary \n",
    "to explicitly form the sample covariance matrices directly. The larger the ensemble the better the results, but even  comparatively small ensembles can still be useful.\n",
    "For example, within modern weather forecasts we have noted that the dimension of the state space might be of order $10^{9}$,  but typical ensemble sizes are only of order 10-100. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72653306",
   "metadata": {},
   "source": [
    "## Further reading\n",
    "\n",
    "These lectures can only provide an incomplete introduction to the topic of data assimilation. For those who wish to know more I recommend the following books:\n",
    "\n",
    "- Wunsch, C., 1996. The ocean circulation inverse problem. Cambridge University Press.\n",
    "- Law, K., Stuart, A. and Zygalakis, K., 2015. Data assimilation. Cham, Switzerland: Springer.\n",
    "- Sanz-Alonso, D., Stuart, A. and Taeb, A., 2023. Inverse problems and data assimilation (Vol. 107). Cambridge University Press."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeoinf-0ZCu7S8P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
