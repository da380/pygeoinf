{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c030612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this when using colab. \n",
    "#%pip install pygeoinf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785a413b",
   "metadata": {},
   "source": [
    "# Tutorial 7 - Bayesian inversions\n",
    "\n",
    "Building on Tutorial 6, we consider in furter detail the solution on our linear inverse problem using Bayesian methods. The set up of the foward problem is identical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb8754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygeoinf as inf\n",
    "from pygeoinf.symmetric_space.circle import Sobolev\n",
    "\n",
    "# Set the Sobolev parameters for the model space. \n",
    "order = 2.0\n",
    "length_scale = 0.05\n",
    "\n",
    "# Set the model space. \n",
    "model_space = Sobolev.from_sobolev_parameters(order, length_scale, power_of_two=True)\n",
    "\n",
    "print(f'The discretised model space has dimension: {model_space.dim}')\n",
    "\n",
    "# Set up the observation points. \n",
    "n = 30\n",
    "observation_points = np.random.uniform(0, 2*np.pi, n)\n",
    "\n",
    "# Set the forward operator\n",
    "forward_operator = model_space.point_evaluation_operator(observation_points)\n",
    "\n",
    "# Set the data space. \n",
    "data_space = forward_operator.codomain\n",
    "\n",
    "print(f'The data space has dimension: {data_space.dim}')\n",
    "\n",
    "# First define a Gaussian measure on the model space. \n",
    "error_length_scale = 0.2\n",
    "error_amplitude = 0.1\n",
    "mu = model_space.heat_gaussian_measure(error_length_scale, error_amplitude)\n",
    "\n",
    "# Push forward the measure under the forward operator. \n",
    "nu = mu.affine_mapping(operator=forward_operator)\n",
    "\n",
    "# Extract the covariance matrix and use this to define the desired measure. \n",
    "covariance_matrix = nu.covariance.matrix(dense=True)\n",
    "data_error_measure = inf.GaussianMeasure.from_covariance_matrix(data_space, covariance_matrix)\n",
    "\n",
    "# Get the standard deviations for each datum for use in plotting. \n",
    "standard_deviations = np.zeros(data_space.dim)\n",
    "for i in range(data_space.dim):\n",
    "    v = data_space.basis_vector(i)\n",
    "    standard_deviations = np.sqrt(data_space.inner_product(data_error_measure.covariance(v), v))\n",
    "\n",
    "# Set the forward problem. \n",
    "forward_problem = inf.LinearForwardProblem(forward_operator, data_error_measure=data_error_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984e9f65",
   "metadata": {},
   "source": [
    "The main new feature within the Bayesian approach to the inverse problem is the specification of a prior measure on the model space. Here we select the expectation of the prior directly and chose its covariance from one the standard options that have been implemented within the ```Sobolev``` class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851b4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model prior expectation. \n",
    "expectation = model_space.project_function(lambda th: 2* np.sin(6*th) * np.exp(-(th-np.pi)**2) )\n",
    "\n",
    "# Set the model prior. \n",
    "covariance_length_scale = 0.05\n",
    "covariance_amplitude = 0.3\n",
    "model_prior_measure = model_space.heat_gaussian_measure(covariance_length_scale, covariance_amplitude, expectation=expectation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685418d5",
   "metadata": {},
   "source": [
    "Using this measure, we can generate a model and corresponding data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e1bd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, data = forward_problem.synthetic_model_and_data(model_prior_measure)\n",
    "\n",
    "# Plot the model and associated data. \n",
    "fig, ax = model_space.plot(model, color=\"k\", figsize=(15, 10))\n",
    "ax.errorbar(observation_points, data, 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062399f2",
   "metadata": {},
   "source": [
    "To set up the ```LinearBayesianInversion``` class we provide the forward problem and the model prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d98431d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "bayesian_inversion = inf.LinearBayesianInversion(forward_problem, model_prior_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd4f3a",
   "metadata": {},
   "source": [
    "It can be shown that the model posterior measure is also Gaussian with expectation\n",
    "$$\n",
    "\\overline{u}_{\\mathrm{post}} = \\overline{u} +  Q A^{*} (AQA^{*} + R)^{-1}(v - A\\overline{u}) \n",
    "$$\n",
    "and covariance\n",
    "$$\n",
    "Q_{\\mathrm{post}} = Q - Q A^{*} (AQA^{*} + R)^{-1} AQ, \n",
    "$$\n",
    "where $\\overline{u}$ is the prior expectation and $Q$ the prior covariance. We call $AQA^{*}+R$ the **normal operator** for this problem. It can be accessed through:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8cad13",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_operator = bayesian_inversion.normal_operator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c749f",
   "metadata": {},
   "source": [
    "Direct access to this operator is potentially useful in building preconditioners. Note that this operator is self-adjoint, positive-definite, and defined on the finite-dimensional model space which, in almost all cases, will be standard Euclidean space.\n",
    "\n",
    "To solve the Bayesian inverse problem we use the method ```model_posterior_measure``` which takes as input the data and a ```LinearSolver``` and returns the desired measure. Optionally, a preconditioner for the normal operator can be provided that can be used in combination with an ```IterativeLinearSolver```.\n",
    "\n",
    "When the dimension of the data space is relatively small, it is often sufficient to use a simple Cholesky solver. This has a comparatively high upfront cost of forming and factoring the dense matrix representation, but once this is done the action of the posterior covariance can be determined at a low cost. \n",
    "\n",
    "In the code below we obtain the posterior measure and compare its expected value to the true model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21162e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the posterior distribution. \n",
    "solver = inf.CholeskySolver()\n",
    "model_posterior_measure = bayesian_inversion.model_posterior_measure(data, solver)\n",
    "\n",
    "# Plot the posterior expectation. \n",
    "# Plot the model and associated data and the least squares solution.\n",
    "fig, ax = model_space.plot(model, color=\"k\", figsize=(15, 10))\n",
    "ax.errorbar(observation_points, data , 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "model_space.plot(model_posterior_measure.expectation, color=\"b\", fig=fig, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28a69d",
   "metadata": {},
   "source": [
    "The posterior measure returned by the ```LinearBayesianInversion``` class does not have a covariance in factored form nor otherwise have a sampling method set. The action of this covariance on any given pair of model vectors can be readily computed, and hence we can explore the uncertainty along directions in the model space. \n",
    "\n",
    "To gain a fuller understanding of the model uncertainty, one option is to compute the posterior covariance in dense form and then factor it so as to allow for sampling. The cost of this calculation will, however, generally be very high as due to the model space dimension. A typically preferable method is based on a low rank Cholesky factorisation of the posterior covariance. This is implemented in the method ```low_rank_approximation``` within the ```GaussianMeasure``` class. The factorisation is done using randomised methods, and can be based on either a fixed or variable rank factorisation. \n",
    "\n",
    "In the code below we use a variable rank factorisation, setting also the relative tolerance in estimating the associated error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19793e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "low_rank_posterior_approximation = model_posterior_measure.low_rank_approximation(10, method=\"variable\", rtol=1e-6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49db842",
   "metadata": {},
   "source": [
    "Once this low rank approximation is formed, we can readily sample from the posterior to learn about data uncertainties. For example, we can plot a set of samples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9261b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = model_space.plot(model, color=\"k\", figsize=(15, 10))\n",
    "ax.errorbar(observation_points, data , 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "\n",
    "number_of_samples = 500\n",
    "samples = low_rank_posterior_approximation.samples(number_of_samples)\n",
    "for sample in samples:    \n",
    "    model_space.plot(sample, color=\"b\", fig=fig, ax=ax, alpha=5/number_of_samples)\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa214428",
   "metadata": {},
   "source": [
    "We can also use a set of such samples to estimate the pointwise standard deviation for the the posterior measure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed19aac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pointwise_variance = low_rank_posterior_approximation.sample_pointwise_variance(number_of_samples)\n",
    "model_pointwise_std = np.sqrt(model_pointwise_variance)\n",
    "\n",
    "\n",
    "# Plot the\n",
    "fig, ax = model_space.plot(model, color=\"k\", figsize=(15,10))\n",
    "ax.errorbar(observation_points, data, 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "model_space.plot(\n",
    "    model_posterior_measure.expectation, fig=fig, ax=ax, color=\"b\"\n",
    ")\n",
    "model_space.plot_error_bounds(\n",
    "    model_posterior_measure.expectation,\n",
    "    2 * model_pointwise_std,\n",
    "    fig=fig,\n",
    "    ax=ax,\n",
    "    alpha=0.2,\n",
    "    color=\"b\",\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e45dc30",
   "metadata": {},
   "source": [
    "A significant appeal of Bayesian methods is that they provide a measure of uncertainty on the models produced. But it must be rememebered that these answers are only as good as the prior probability distribution, and that within inverse problems there is rarely enough data for the choice of prior to be immaterial. \n",
    "\n",
    "In the code below, we repeat the calculations above, using the same model and data, but now using a different prior. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054bf7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the new model prior. \n",
    "new_covariance_length_scale = 0.1\n",
    "new_covariance_amplitude = 0.1\n",
    "new_model_prior_measure = model_space.heat_gaussian_measure(new_covariance_length_scale, new_covariance_amplitude)\n",
    "\n",
    "# Set up the inversion and form the posterior.\n",
    "new_bayesian_inversion = inf.LinearBayesianInversion(forward_problem, new_model_prior_measure)\n",
    "new_model_posterior_measure = new_bayesian_inversion.model_posterior_measure(data, solver)\n",
    "\n",
    "# Form the low rank approximation\n",
    "new_low_rank_posterior_approximation = new_model_posterior_measure.low_rank_approximation(10, method=\"variable\", rtol=1e-6)\n",
    "\n",
    "# Plot the posterior expectation with uncertainty bounds.\n",
    "\n",
    "new_model_pointwise_variance = new_low_rank_posterior_approximation.sample_pointwise_variance(number_of_samples)\n",
    "new_model_pointwise_std = np.sqrt(new_model_pointwise_variance)\n",
    "\n",
    "\n",
    "# Plot the\n",
    "fig, ax = model_space.plot(model, color=\"k\", figsize=(15,10))\n",
    "ax.errorbar(observation_points, data, 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "model_space.plot(\n",
    "    new_model_posterior_measure.expectation, fig=fig, ax=ax, color=\"b\"\n",
    ")\n",
    "model_space.plot_error_bounds(\n",
    "    new_model_posterior_measure.expectation,\n",
    "    2 * new_model_pointwise_std,\n",
    "    fig=fig,\n",
    "    ax=ax,\n",
    "    alpha=0.2,\n",
    "    color=\"b\",\n",
    ")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeoinf-0ZCu7S8P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
