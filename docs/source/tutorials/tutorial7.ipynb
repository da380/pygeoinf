{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "634e3ce9",
   "metadata": {},
   "source": [
    "# Tutorial 7: Linear Optimisation Methods\n",
    "\n",
    "In this tutorial, we'll take a closer look at solving inverse problems using linear optimization. Unlike the Bayesian methods, which seek a full probability distribution, these methods aim to find a single, optimal \"best-fit\" model.\n",
    "\n",
    "We will revisit the problem from Tutorial 1: estimating a smooth function on a circle from sparse, noisy point data.\n",
    "\n",
    "The two main methods we'll explore are:\n",
    "1.  **Tikhonov-Regularized Least Squares:** This method balances fitting the data and controlling the norm of the solution via a manually chosen damping parameter, `α`.\n",
    "2.  **Minimum Norm (Discrepancy Principle):** This method automatically finds the optimal damping parameter `α` such that the data is fit to a statistically acceptable level.\n",
    "\n",
    "Our goal is to understand how these methods work and see the effect of the regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafdf684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygeoinf as inf\n",
    "from pygeoinf.symmetric_space.circle import Sobolev, CircleHelper\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ec0e2",
   "metadata": {},
   "source": [
    "## 1. Setting up the Problem\n",
    "\n",
    "First, let's set up the same inverse problem as in Tutorial 1. We have a `Sobolev` space for our model, a `point_evaluation_operator` as our forward map, and a `GaussianMeasure` for the data noise. We then generate a \"true\" model and synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beddd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup the forward problem (same as Tutorial 1) ---\n",
    "model_space = Sobolev.from_sobolev_parameters(2.0, 0.05)\n",
    "n_data = 20\n",
    "observation_points = model_space.random_points(n_data)\n",
    "forward_operator = model_space.point_evaluation_operator(observation_points)\n",
    "data_space = forward_operator.codomain\n",
    "standard_deviation = 0.1\n",
    "data_error_measure = inf.GaussianMeasure.from_standard_deviation(\n",
    "    data_space, standard_deviation\n",
    ")\n",
    "forward_problem = inf.LinearForwardProblem(\n",
    "    forward_operator, data_error_measure=data_error_measure\n",
    ")\n",
    "\n",
    "# --- Generate synthetic data ---\n",
    "model_prior_measure = model_space.point_value_scaled_heat_kernel_gaussian_measure(\n",
    "    scale=0.1, amplitude=1.0\n",
    ")\n",
    "true_model, data = forward_problem.synthetic_model_and_data(model_prior_measure)\n",
    "\n",
    "print(\"Forward problem and synthetic data are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be420e7f",
   "metadata": {},
   "source": [
    "## 2. Method 1: Tikhonov-Regularized Least Squares\n",
    "\n",
    "The `LinearLeastSquaresInversion` method solves the problem by minimizing the Tikhonov functional:\n",
    "\n",
    "$$\n",
    "J(u) = \\|A(u) - d\\|^2_{C_e^{-1}} + \\alpha^2 \\|u\\|^2\n",
    "$$\n",
    "\n",
    "Here, `α` is the **damping parameter**. It controls the trade-off:\n",
    "* A **small `α`** trusts the data more, leading to a solution with a small data misfit but potentially a large norm (and oscillatory artifacts).\n",
    "* A **large `α`** trusts the prior knowledge more (that the solution should be \"small\"), leading to a smooth, low-norm solution that may not fit the data well.\n",
    "\n",
    "The choice of `α` is critical and often requires experimentation. Let's solve the problem for three different values of `α`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f471e256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the inversion object\n",
    "lsq_inversion = inf.LinearLeastSquaresInversion(forward_problem)\n",
    "solver = inf.CGSolver(rtol=1e-9)\n",
    "\n",
    "# --- Solve for different damping parameters ---\n",
    "damping_values = [ 1e-4, 1e0, 1e4]\n",
    "lsq_solutions = {}\n",
    "\n",
    "for alpha in damping_values:\n",
    "    # Get the operator that maps data to the least-squares solution for this alpha\n",
    "    lsq_operator = lsq_inversion.least_squares_operator(alpha, solver)\n",
    "    solution = lsq_operator(data)\n",
    "    lsq_solutions[alpha] = solution\n",
    "\n",
    "# --- Plot the results ---\n",
    "fig, ax = model_space.plot(true_model, color=\"k\", linestyle=\"--\", label=\"True Model\", figsize=(15, 10))\n",
    "ax.errorbar(observation_points, data, 2 * standard_deviation, fmt=\"ko\", capsize=3, label=\"Data\")\n",
    "\n",
    "colors = ['r', 'g', 'b']\n",
    "for (alpha, solution), color in zip(lsq_solutions.items(), colors):\n",
    "    model_space.plot(solution, fig=fig, ax=ax, color=color, label=f'LSQ Solution (α={alpha})')\n",
    "\n",
    "ax.set_title(\"Least-Squares Solutions for Different Damping Parameters\", fontsize=16)\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89aab8b",
   "metadata": {},
   "source": [
    "As you can see, the very small damping parameter (`α=1e-4`) \"over-fits\" the noisy data, leading to an oscillatory solution. The very large damping parameter (`α=1e4`) \"under-fits\" the data, leading to a solution that is too smooth and close to zero. The intermediate value (`α=1e0`) appears to be a good compromise.\n",
    "\n",
    "This highlights the main challenge of this method: choosing the right `α`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60318e1d",
   "metadata": {},
   "source": [
    "## 3. Method 2: Minimum Norm Solution via Discrepancy Principle\n",
    "\n",
    "The `LinearMinimumNormInversion` method provides an automated way to choose the damping parameter. It uses the **discrepancy principle**, which aims to find the model with the *smallest possible norm* that still fits the data to a statistically plausible degree.\n",
    "\n",
    "It works by finding the `α` such that the chi-squared misfit is equal to its expected value (the number of data points):\n",
    "$$\n",
    "\\chi^2(u_\\alpha, d) \\approx N_{data}\n",
    "$$\n",
    "This is done internally using a bracketing search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203dbc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the inversion method\n",
    "min_norm_inversion = inf.LinearMinimumNormInversion(forward_problem)\n",
    "\n",
    "# Get the operator that maps data to the solution.\n",
    "# This operator will contain the logic to automatically find the best damping.\n",
    "min_norm_operator = min_norm_inversion.minimum_norm_operator(solver)\n",
    "\n",
    "# Compute the minimum norm model\n",
    "min_norm_model = min_norm_operator(data)\n",
    "\n",
    "# --- Plot the results and compare ---\n",
    "fig, ax = model_space.plot(true_model, color=\"k\", linestyle=\"--\", label=\"True Model\", figsize=(15, 10))\n",
    "ax.errorbar(observation_points, data, 2 * standard_deviation, fmt=\"ko\", capsize=3, label=\"Data\")\n",
    "\n",
    "# Plot the best least-squares solution from before for comparison\n",
    "model_space.plot(lsq_solutions[1], fig=fig, ax=ax, color='g', linestyle='-.', label='Best LSQ (α=0.1)')\n",
    "\n",
    "# Plot the new minimum norm solution\n",
    "model_space.plot(min_norm_model, fig=fig, ax=ax,  color='b', label='Minimum Norm Solution')\n",
    "\n",
    "ax.set_title(\"Comparison of Least-Squares and Minimum Norm Solutions\", fontsize=16)\n",
    "ax.legend()\n",
    "ax.grid(True, linestyle=\":\", alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de34e59b",
   "metadata": {},
   "source": [
    "The minimum norm solution is very similar to our \"best guess\" from the manual Tikhonov method. This demonstrates its power: it provides a robust, automated way to achieve a well-regularized solution without the need for manual parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5867692f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeoinf-0ZCu7S8P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
