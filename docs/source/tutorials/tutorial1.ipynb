{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfbe1016",
   "metadata": {},
   "source": [
    "# Tutorial 1: A First Example - Function Inversion on a Circle\n",
    "\n",
    "The aim of this tutorial is to show how this library can be used to solve a simple, but non-trivial, linear inverse problem from start to finish. We will not explain every detail, as later tutorials will build up the various ideas.\n",
    "\n",
    "### The Problem\n",
    "\n",
    "We consider a smooth function, $u$, defined on the unit circle, $\\mathbb{S}^{1}$. We are given $n$ noisy point measurements of this function:\n",
    "\n",
    "$$\n",
    "d_{i} = u(\\theta_{i}) + e_{i}\n",
    "$$\n",
    "\n",
    "where the $\\theta_{i}$ are known locations and the $e_{i}$ are normally distributed random errors. From this data, we wish to estimate the unknown function, $u$.\n",
    "\n",
    "We will solve this problem using two different methods: a **Minimum Norm Inversion** and a full **Bayesian Inversion**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedea187",
   "metadata": {},
   "source": [
    "## 1. Defining the Problem Components\n",
    "\n",
    "First, we define the three key components of any inverse problem:\n",
    "1.  **The Model Space:** The `HilbertSpace` our unknown function `u` lives in. We'll use a `Sobolev` space, which is a space of smooth functions.\n",
    "2.  **The Forward Operator:** The `LinearOperator` that maps a model `u` to the \"perfect\" data `d_perfect = A(u)`. Here, this is the `point_evaluation_operator`.\n",
    "3.  **The Data Error:** A `GaussianMeasure` that describes the statistics of the noise `e`.\n",
    "\n",
    "These are then bundled into a `LinearForwardProblem` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a0eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run in colab, uncomment the line below to install pygeoinf. \n",
    "#%pip install pygeoinf\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygeoinf as inf\n",
    "from pygeoinf.symmetric_space.circle import Sobolev, CircleHelper\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# 1. Set up the model space\n",
    "# We use a helper method to automatically determine the necessary resolution (kmax)\n",
    "# from the desired physical properties: a smoothness order of 2.0 and a\n",
    "# correlation length-scale of 0.05.\n",
    "model_space = Sobolev.from_sobolev_parameters(2.0, 0.05)\n",
    "\n",
    "# 2. Define the forward operator\n",
    "# This operator will take a function from our model_space and evaluate it\n",
    "# at `n` random points on the circle.\n",
    "n_data = 20\n",
    "observation_points = model_space.random_points(n_data)\n",
    "forward_operator = model_space.point_evaluation_operator(observation_points)\n",
    "data_space = forward_operator.codomain\n",
    "\n",
    "# 3. Define the data error measure\n",
    "# We assume the noise has a standard deviation of 0.1.\n",
    "standard_deviation = 0.1\n",
    "data_error_measure = inf.GaussianMeasure.from_standard_deviation(\n",
    "    data_space, standard_deviation\n",
    ")\n",
    "\n",
    "# Bundle everything into a forward problem object\n",
    "forward_problem = inf.LinearForwardProblem(\n",
    "    forward_operator, data_error_measure=data_error_measure\n",
    ")\n",
    "\n",
    "print(f\"Model space dimension (kmax): {model_space.kmax}\")\n",
    "print(f\"Data space dimension: {data_space.dim}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cacbb145",
   "metadata": {},
   "source": [
    "## 2. Generating Synthetic Data\n",
    "\n",
    "To test our inversion methods, we need a \"true\" model and a corresponding set of noisy data. We create the true model by drawing a random sample from a \"prior\" `GaussianMeasure`. This prior represents our initial guess about what the function might look like (in this case, a smooth function with a typical amplitude of 1.0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0695fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prior measure on the model space.\n",
    "# We use a heat kernel covariance, which produces smooth, random functions.\n",
    "model_prior_measure = model_space.point_value_scaled_heat_kernel_gaussian_measure(0.1, 1.0)\n",
    "\n",
    "# Generate the true model and the corresponding noisy data.\n",
    "true_model, data = forward_problem.synthetic_model_and_data(model_prior_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c602d3",
   "metadata": {},
   "source": [
    "### A Helper Function for Plotting\n",
    "\n",
    "To avoid repeating code, let's create a helper function to visualize our results. It will plot the true model, the noisy data, the inversion result, and an optional uncertainty bound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b911f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d68cf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(\n",
    "    space: CircleHelper,\n",
    "    true_model: np.ndarray,\n",
    "    data: np.ndarray,\n",
    "    obs_points: np.ndarray,\n",
    "    data_std: float,\n",
    "    solution_model: np.ndarray,\n",
    "    solution_label: str,\n",
    "    solution_std: np.ndarray = None,\n",
    "):\n",
    "    \"\"\"Helper function to create a consistent plot.\"\"\"\n",
    "    fig, ax = space.plot(true_model, color=\"k\", linestyle=\"--\", label=\"True Model\", figsize=(15, 10))\n",
    "    \n",
    "    # Plot the solution\n",
    "    space.plot(solution_model, fig=fig, ax=ax, color=\"b\", label=solution_label)\n",
    "    \n",
    "    # Plot uncertainty bounds if provided\n",
    "    if solution_std is not None:\n",
    "        space.plot_error_bounds(\n",
    "            solution_model, 2 * solution_std, fig=fig, ax=ax, alpha=0.2, color=\"b\"\n",
    "        )\n",
    "        \n",
    "    # Plot the noisy data points\n",
    "    ax.errorbar(obs_points, data, 2 * data_std, fmt=\"ko\", capsize=3, label=\"Data\")\n",
    "    \n",
    "    ax.set_title(\"Inversion Results\", fontsize=16)\n",
    "    ax.set_xlabel(\"Angle (radians)\")\n",
    "    ax.set_ylabel(\"Function Value\")\n",
    "    ax.legend()\n",
    "    ax.grid(True, linestyle=\":\", alpha=0.7)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot our initial ground truth and data\n",
    "plot_results(model_space, true_model, data, observation_points, standard_deviation,\n",
    "             solution_model=np.zeros_like(true_model), solution_label=\"Initial State (Zero)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324ba049",
   "metadata": {},
   "source": [
    "## 3. Method 1: Minimum Norm Solution\n",
    "\n",
    "Our first approach is the **minimum norm solution**. Of all the models that fit the data acceptably (according to a chi-squared test), this method finds the unique one with the smallest Sobolev norm. It yields a single, deterministic estimate of the model.\n",
    "\n",
    "We use `CGSolver` because the normal equations for this problem are posed on the high-dimensional model space. A matrix-free iterative solver is much more efficient than forming a large, dense matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1d4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the inversion method\n",
    "minimum_norm_inversion = inf.LinearMinimumNormInversion(forward_problem)\n",
    "\n",
    "# Get the operator that maps data to the solution.\n",
    "solver = inf.CGSolver()\n",
    "minimum_norm_operator = minimum_norm_inversion.minimum_norm_operator(solver)\n",
    "\n",
    "# Compute the minimum norm model\n",
    "minimum_norm_model = minimum_norm_operator(data)\n",
    "\n",
    "# Plot the result\n",
    "plot_results(\n",
    "    model_space,\n",
    "    true_model,\n",
    "    data,\n",
    "    observation_points,\n",
    "    standard_deviation,\n",
    "    solution_model=minimum_norm_model,\n",
    "    solution_label=\"Minimum Norm Solution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e3e5d0",
   "metadata": {},
   "source": [
    "## 4. Method 2: Bayesian Inversion\n",
    "\n",
    "For the second method, we perform a **Bayesian inversion**. This requires a prior measure, and we'll use the same one we used to generate our true model. Instead of a single solution, this method returns a full posterior probability distribution (`GaussianMeasure`), which includes both a best-estimate (the posterior mean) and a quantification of uncertainty (the posterior covariance).\n",
    "\n",
    "Since the full posterior covariance operator is too large to store, we create a low-rank approximation of it. We can then draw samples from this approximation to estimate the pointwise uncertainty.\n",
    "\n",
    "The key matrix to invert in the Bayesian case lives on the low-dimensional data space. Since we only have 20 data points, a direct `CholeskySolver` is very fast and efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b99994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Bayesian inversion method\n",
    "bayesian_inversion = inf.LinearBayesianInversion(forward_problem, model_prior_measure)\n",
    "\n",
    "# Compute the posterior distribution\n",
    "solver = inf.CholeskySolver(galerkin=True)\n",
    "model_posterior_measure = bayesian_inversion.model_posterior_measure(data, solver)\n",
    "posterior_mean = model_posterior_measure.expectation\n",
    "\n",
    "# Estimate the pointwise variance from a low-rank approximation\n",
    "low_rank_posterior = model_posterior_measure.low_rank_approximation(\n",
    "    10, method=\"variable\", rtol=1e-4\n",
    ")\n",
    "posterior_pointwise_variance = low_rank_posterior.sample_pointwise_variance(200)\n",
    "posterior_std = np.sqrt(posterior_pointwise_variance)\n",
    "\n",
    "# Plot the posterior mean and its 95% confidence interval\n",
    "plot_results(\n",
    "    model_space,\n",
    "    true_model,\n",
    "    data,\n",
    "    observation_points,\n",
    "    standard_deviation,\n",
    "    solution_model=posterior_mean,\n",
    "    solution_label=\"Posterior Mean\",\n",
    "    solution_std=posterior_std,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a85096",
   "metadata": {},
   "source": [
    "## 5. Conclusion\n",
    "\n",
    "As you can see, both methods do a good job of recovering the true model from the sparse, noisy data.\n",
    "\n",
    "* The **Minimum Norm Solution** provides a single, good-looking estimate.\n",
    "* The **Bayesian Inversion** provides a similar estimate (the posterior mean) but also gives us the credible interval (the blue shaded region), which tells us where our estimate is most and least certain. This uncertainty quantification is a key advantage of the Bayesian approach, but one that depends on the prior information being suitable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeoinf-0ZCu7S8P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
