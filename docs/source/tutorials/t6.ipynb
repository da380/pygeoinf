{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dc3406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this when using colab. \n",
    "#%pip install pygeoinf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9ddffc",
   "metadata": {},
   "source": [
    "# Tutorial 6 - Minumum norm inversions\n",
    "We now return to the inverse problem illustrated in outline within Tutorial 1 explaining the ideas and methods in fuller detail. In this tutorial we restrict attention to the solution of the inverse problem using the minimum norm method, with the Bayesian approach discussed in Tutorial 7. \n",
    "\n",
    "### Setting up the forward problem\n",
    "\n",
    "Recall that within our problem we are given data\n",
    "$$\n",
    "v_{i} = u(\\theta_{i}) +z_{i}, \\quad i = 1,\\dots, n,\n",
    "$$\n",
    "where $u \\in H^{s}(\\mathbb{S}^{1})$ for $s > 1/2$ is an unknown function, the $\\theta_{i}$ are given angles, and $z_{i}$ random errors. While the constraint on the Sobolev exponent is necessary for point-evaluation to be well-defined, neither the precise value of $s$ nor of the length-scale paramater $\\lambda>0$ occurring within the definition of the Sobolev space are provided. As a result, we should examine later how these choices effect the solutions obtained. \n",
    "\n",
    "For the random errors, we will assume more concretely that the $z_{i}$ are components of a random vector $z \\sim \\mathcal{N}_{\\mathbb{R}^{n}}(0, R)$. In Tutorial 1 we took the covariance matrix to be proportional to the identity (and hence used ```from_standard_deviation``` to construct the associated ```GaussianMeasure```), but now we allow it to take a more general form. While the expectation of the error term is still taken to be zero - and in practice this the data can always be corrected such that this is true - the methods discussed can also handle the more general case if desired. \n",
    "\n",
    "\n",
    "To proceed, we first set up the model space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9b4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pygeoinf as inf\n",
    "from pygeoinf.symmetric_space.circle import Sobolev\n",
    "\n",
    "# Set the Sobolev parameters for the model space. \n",
    "order = 2.0\n",
    "length_scale = 0.1\n",
    "\n",
    "# Set the model space. \n",
    "model_space = Sobolev.from_sobolev_parameters(order, length_scale, power_of_two=True)\n",
    "\n",
    "print(f'The discretised model space has dimension: {model_space.dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2f2015",
   "metadata": {},
   "source": [
    "Note that we have used the static method ```from_sobolev_parameters``` to select an appropriate discretisation of the model space. In this case, we have set the optional argument ```power_of_two``` to be ```True```. This ensures that the discretised model space has a dimension $2^{m}$ for some $m$. Doing this aids efficiency within fast Fourier transformations. \n",
    "\n",
    "To get the forward operator, we can use the method ```point_evaluation_operator``` that is built into the Sobolev class. To form this operator, we just need to provide a list of the observation points and the corresponding ```LinearOperator``` is returned. The codomain of this operator is the appropriately dimensioned ```EuclideanSpace``` which we store as the data space for our problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the observation points. \n",
    "n = 15\n",
    "observation_points = np.random.uniform(0, 2*np.pi, n)\n",
    "\n",
    "# Set the forward operator\n",
    "forward_operator = model_space.point_evaluation_operator(observation_points)\n",
    "\n",
    "# Set the data space. \n",
    "data_space = forward_operator.codomain\n",
    "\n",
    "print(f'The data space has dimension: {data_space.dim}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fba8c23",
   "metadata": {},
   "source": [
    "We now need to define the data error measure. To proceed, we will assume that these errors arise from an underlying Gaussian measure, with this being a convenient method for generating errors with a non-diagonal covariance matrix. In code below we first set up such a measure on the model space with non-zero mean. This can be pushed forward using the forward operator to form the data error measure. An issue, however, is that the resulting measure does not have its inverse covariance defined, nor a sampling method set up, both of which we later require. To proceed, we can extract the measure's covariance matrix in dense form and use this to construct our final data error measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbece92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First define a Gaussian measure on the model space. \n",
    "error_length_scale = 0.2\n",
    "error_amplitude = 0.1\n",
    "mu = model_space.heat_gaussian_measure(error_length_scale, error_amplitude)\n",
    "\n",
    "# Push forward the measure under the forward operator. \n",
    "nu = mu.affine_mapping(operator=forward_operator)\n",
    "\n",
    "# Extract the covariance matrix and use this to define the desired measure. \n",
    "covariance_matrix = nu.covariance.matrix(dense=True)\n",
    "data_error_measure = inf.GaussianMeasure.from_covariance_matrix(data_space, covariance_matrix)\n",
    "\n",
    "# Get the standard deviations for each datum for use in plotting. \n",
    "standard_deviations = np.zeros(data_space.dim)\n",
    "for i in range(data_space.dim):\n",
    "    v = data_space.basis_vector(i)\n",
    "    standard_deviations = np.sqrt(data_space.inner_product(data_error_measure.covariance(v), v))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37882be7",
   "metadata": {},
   "source": [
    "We are now in a position to set up our forward problem, this being described by an instance of the class ```LinearForwardProblem```. Construction of this object requires us to pass the ```forward_operator``` and the ```data_error_measure```. The second argument is optional with default ```None``` in which case the data are assumed to be error-free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f1e860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the forward problem. \n",
    "forward_problem = inf.LinearForwardProblem(forward_operator, data_error_measure=data_error_measure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68e1f37",
   "metadata": {},
   "source": [
    "The ```LinearForwardProblem``` class has, in of itself, relatively little functionality. It does, however, provides useful methods for generating synthetic data. One is called ```synthetic_data```, takes in a model as argument, and returns the corresponding data. If a data error measure has been set, then sampled values for the data errors are added to the result of the forward operator. Below we directly define a model and use ```synthetic_data``` to generate corresponding data. We also make use of a simple method within the  ```Sobolev``` class for plotting its elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98f0c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the function directly. \n",
    "model = model_space.project_function(lambda th: 2* np.sin(6*th) * np.exp(-(th-np.pi)**2) )\n",
    "\n",
    "# Generate the synthetic data\n",
    "data = forward_problem.synthetic_data(model)\n",
    "\n",
    "# Plot the model and associated data. \n",
    "fig, ax = model_space.plot(model, color=\"k\", figsize=(15, 10))\n",
    "ax.errorbar(observation_points, data, 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5db28d",
   "metadata": {},
   "source": [
    "The ```LinearForwardProblem``` class also provided methods related to the chi-squared statistic that can be used to assess the compatibility of a given model with data. To use these methods it is necessary that a data error measure is provided and that it has its inverse covariance set. These ideas are illustrated below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired significance level\n",
    "significance_level = 0.95\n",
    "\n",
    "# Get the critical chi-squared value.\n",
    "critical_chi_squared = forward_problem.critical_chi_squared(significance_level)\n",
    "print(f'The critical chi-squared value is: {critical_chi_squared}')\n",
    "\n",
    "# Get the chi-squared for the true model\n",
    "chi_squared = forward_problem.chi_squared(model, data)\n",
    "print(f'The chi-squared for the true model is: {chi_squared}')\n",
    "\n",
    "# Check if the model is compatible with the data. \n",
    "print(f'Is the chi-squared test passed for the true model: {forward_problem.chi_squared_test(significance_level, model, data)}')\n",
    "\n",
    "\n",
    "# Repeat the checks using a difference model \n",
    "new_model = 2 * model\n",
    "chi_squared = forward_problem.chi_squared(new_model, data)\n",
    "print(f'The chi-squared for the new model is: {chi_squared}')\n",
    "print(f'Is the chi-squared test passed for the new model: {forward_problem.chi_squared_test(significance_level, new_model, data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee810ac1",
   "metadata": {},
   "source": [
    "### Least squares inversions\n",
    "\n",
    "As a step towards understanding minimum norm solutions of the inverse problem, we begin with the **regularised  least squares method**. The aim here is to minimise\n",
    "$$\n",
    "J = \\left(R^{-1}(Au  - v),Au - v \\right)_{Y} + \\mu\\|u\\|_{X}^{2}, \n",
    "$$\n",
    "where we write $X$ for the model space, $Y$ for the data space, and $\\mu>0$ is a damping parameter. Note that the first term is equal to the\n",
    "$\\chi^{2}$ statistic.  This quadratic functional has a unique minimum point\n",
    "at the model defined through the  **normal equations**:\n",
    "$$\n",
    "(A^{*}R^{-1}A+\\mu) u = A^{*}R^{-1}v. \n",
    "$$\n",
    " \n",
    "\n",
    "This problem can be solved using the ```LinearLeastSquaresInversion``` class. To construct an instance, we need only provide the forward problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b925b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "least_squares_inversion = inf.LinearLeastSquaresInversion(forward_problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ccc370",
   "metadata": {},
   "source": [
    "From this object we can, for example, obtain the normal operator, $A^{*}R^{-1}A+\\mu$ for a given damping parameter. Extraction of this operator can be useful when, for example, building a pre-conditioner for the solution of the normal equations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e5997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "damping_parameter = 0.1\n",
    "normal_operator = least_squares_inversion.normal_operator(damping_parameter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c87a19",
   "metadata": {},
   "source": [
    "To obtain the least squares solution we need to solve a linear equation involving the normal operator. To proceed, we need to introduce the ```LinearSolver``` class. Each linear solver is derived from this abstract base class and must provide  a ```__call__``` method that takes in a ```LinearOperator``` and outputs its inverse as  a new ```LinearOperator```. There are two types of ```LinearSolver```, namely ```DirectLinearSolver``` and ```IterativeLinearSolver```. \n",
    "\n",
    "\n",
    "In the case of ```DirectLinearSolver``` a dense matrix representation of the operator is formed and used to constructe the inverse operator. This can either be based on the standard or Galerkin matrix representation. The methods currently implemented are:\n",
    "\n",
    "- ```LUSolver```: Based on the ```scipy``` implementation of LU factorisation and backsubstitution. Applies to general operators. \n",
    "- ```CholeskySolver```: Based on the ```scipy``` implementation of Cholesky factorisation and backsubstitution. Requires the operator's matrix representation be symmetric and positive-definite. \n",
    "\n",
    "\n",
    "Solvers derived from ```IterativeLinearSolver``` are matrix-free and generally better for linear equations on high-dimensional spaces. The ```__call__``` method in these cases can optionally be provided with a predconditioner, this being a ```LinearOperator``` that acts as an approximate inverse of the desired operator. The iterative solvers available are:\n",
    "\n",
    "- ```CGMatrixSolver```: Based on the ```scipy``` implementation of the conjugate gradient method. Requires the operator's matrix representation be symmetric and positive-definite. \n",
    "- ```BICGMatrixSolver```: Based on the ```scipy``` implementation of the biconjugate gradient method. \n",
    "- ```BICGStabMatrixSolver```: Based on the ```scipy``` implementatoin of the stabilised biconjugate gradient method. \n",
    "- ```GMRESMatrixSolver```: Based on the ```scipy``` implementation of the generalised minimum residual method. \n",
    "- ```CGSolver```: An implementation of the conjugate gradient method directly on the operators domain. Requires the operator to be self-adjoint and positive-definite. \n",
    "\n",
    "Note that the methods based on ```scipy``` solvers rely on a matrix-representation of the operator, but do not need this to be formed as a dense matrix. \n",
    "\n",
    "In the code below, we determine the solution of the least squares problem using either a ```CholeskySolver``` or the matrix-free ```CGSolver``` method. Note that for the Cholesky method we set the optional ```galerkin``` argument to ```True```. This is to ensure that the matrix representation used is symmetric and positive-definite. \n",
    "\n",
    "It should be seen that the iterative method is faster and is more robust for small values of the damping parameter (in which case the operator becomes poorly conditioned). Indeed, it is important that ```CGSolver``` works directly on the Sobolev space using the correct inner product. This leads to a far better conditioned linear system that those that rely on either matrix representation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5f9759",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the solution method \"direct\" or \"iterative\"\n",
    "method = \"iterative\"\n",
    "\n",
    "if method == \"direct\":\n",
    "    # Set up the Cholesky solver based on the Galerkin representation. \n",
    "    solver = inf.CholeskySolver(galerkin=True)\n",
    "elif method == \"iterative\":\n",
    "    # Set up the Conjugate gradient solver directly on the model space. rtol is \n",
    "    # the relative tolerance applied, with the default being 1e-5. \n",
    "    solver = inf.CGSolver(rtol=1e-8)    \n",
    "else:\n",
    "    raise ValueError(\"Invald option\")\n",
    "\n",
    "# Form the least squares operator. Note the use of the Galerkin matrix representation. \n",
    "damping_parameter = 10\n",
    "least_squares_operator = least_squares_inversion.least_squares_operator(damping_parameter,solver)\n",
    "\n",
    "# Obtain the solution. \n",
    "least_squares_model = least_squares_operator(data)\n",
    "\n",
    "# Plot the model and associated data and the least squares solution.\n",
    "fig, ax = model_space.plot(model, color=\"k\", figsize=(15, 10))\n",
    "ax.errorbar(observation_points, data , 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "model_space.plot(least_squares_model, color=\"b\", fig=fig, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f3e295",
   "metadata": {},
   "source": [
    "It is worth noting that the method ```least_squares_operator``` returns an instance of the ```Operator``` class that is used for general mappings between Hilbert spaces. The ```LinearOperator``` class is derived from ```Operator``` and has significantly more strucutre. The reason is that the solution of the least squares problem does not depend linearly on the data. Rather, the relationship is affine, with the translation term being associated with the expectation of the data error measure. In the error-free case, a ```LinearOperator``` is returned by this method. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdec789c",
   "metadata": {},
   "source": [
    "### Minimum norm inversions\n",
    "\n",
    "A well-known issue with regularised least squares is how an appropriate value for the damping parameter should be selected. Various methods have been proposed to do this but ultimately the question is not sensible. There is no *best* choice of damping parameter without additional information being provided. \n",
    "\n",
    "An alternative method that is more satisfactory is to seek a **minimum norm solution** of the problem. In outline, this method seeks from amougst all those models that are statistically compatible with the data the one with the smallest norm. It can be shown that if there are *any* models that are compatible with the data, then there is a unique one with minimum norm. Note that an equivalent name for this method is an **Occam inversion** as discussed by Constable, Parker, and Constable (1987).\n",
    "\n",
    "\n",
    "Determination of the minimum norm soltuion proceeds as follows:\n",
    "\n",
    "- A significance level, $\\alpha$, is selected for the $\\chi^{2}$-test. From this a critical value, $\\chi_{0}^{2}$, of the test statistic is determined, with a model being compatible with the data if and only if its $\\chi^{2}$-value is less than or equal to $\\chi_{0}^{2}$.\n",
    "- The zero-model is tested for compatibility with the data. If its $\\chi^{2}$ is sufficient small then it is the unique minimum norm model. \n",
    "- Otherwise, the minimum norm model is one whose $\\chi^{2}$-value is equal to the critical value. \n",
    "\n",
    "To determine the model within the third step, we can apply the method of Lagrange multipliers, resulting in the Lagrangian:\n",
    "$$\n",
    "L = \\|u\\|_{X}^{2} + \\xi\\left[\n",
    "\\left(R^{-1}(Au- v),Au  - v \\right)_{Y} - \\chi_{0}^{2}, \n",
    "\\right]\n",
    "$$\n",
    "with $\\xi$ a Lagrange multiplier. The resulting stationarity conditions are:\n",
    "$$\n",
    "(A^{*}R^{-1}A + \\xi^{-1})u = A^{*}R^{-1}v, \\quad \\left(R^{-1}(Au  - v),Au- v \\right)_{Y} = \\chi_{0}^{2}.\n",
    "$$\n",
    "For fixed $\\xi$, the first equation is identical to the least squares normal equations, with $\\xi^{-1}$ being the damping parameter. This allows us to solve for $u$ in terms of $\\xi$, and hence reduce the second constraint into a scalar equation for $\\xi$. Importantly, the resulting function is monotonically decreasing, and hence if it has a solution it can be readily founded using a simple bracketing scheme. \n",
    "\n",
    "This process is carried out within the ```LinearMinimumNormInvesion``` class whose use is illustrated below. Note again that the resulting operator that maps from the data to the model space is non-linear and hence an instance of the ```Operator``` class. In this case, additional non-linearity comes from the fact that the optimal value for $\\xi$ is dependent on the data. \n",
    "\n",
    "Internally, the the action of the  ```minimum_norm_operator``` requires the solution of multiple regularised least squares problems. This is best done with an iterative method that uses the solution of the previous problem as an initial guess. The latter process is facilitated by the ```solve_linear_system``` method within ```IterativeLinearSolvers``` which allows an initial guess to be passed through to the iterative method in addition to the right hand side. \n",
    "\n",
    "A key point about the minimum norm method is that it can fail to find a suitable model. This occurs when no element of the model space fits the data acceptably. In practice, this means that as the Lagrange multiplier tends to infinity the associated value of $\\chi^{2}$ never drops below the critical value. If the method is run with a 95% significance level, and if our error model is correct, this should happen aroud 5% of the time. In such cases the ```minimum_norm_operator``` raises a ```RuntimeError```. We account for this below through a simple use of exception handling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887acfa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the inversion scheme. \n",
    "minimum_norm_inversion = inf.LinearMinimumNormInversion(forward_problem)\n",
    "\n",
    "# Set the necessary options. \n",
    "significance_level = 0.95\n",
    "solver = inf.CGSolver(rtol=1e-8)\n",
    "\n",
    "# Set up the operator from the data to model spaces. \n",
    "minimum_norm_operator = minimum_norm_inversion.minimum_norm_operator(solver, significance_level=significance_level)\n",
    "\n",
    "# Compute and plot the solution. \n",
    "try: \n",
    "    minimum_norm_model = minimum_norm_operator(data)\n",
    "\n",
    "    # Plot the model and associated data and the least squares solution.\n",
    "    fig, ax = model_space.plot(model, color=\"k\", figsize=(15, 10))\n",
    "    ax.errorbar(observation_points, data , 2 * standard_deviations, fmt=\"ko\", capsize=2)\n",
    "    model_space.plot(minimum_norm_model, color=\"b\", fig=fig, ax=ax)\n",
    "    plt.show()\n",
    "\n",
    "except RuntimeError:\n",
    "    print(f'Minimum norm solution does not exist')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b3dc5",
   "metadata": {},
   "source": [
    "A clear advantage of the minimum norm solution is the ellimination of the damping parameter. Nonetheless, this is still just one possible solution that is compatible with the data and so it is ultimately of limited value. Later, we will see how minimum norm solutions can be adapted and applied usefully in the solution of associated inference problems. It is also worth emphasising that the minimum norm solutions are defined relative to a particular choice of model space. If, for example, the Sobolev exponent or length-scale is changed, then the minimum norm solution will change correspondingly. These values can be altered and this notebook rerun to see this in practice. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pygeoinf-0ZCu7S8P-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
